{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24334,"status":"ok","timestamp":1746835320115,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"Q3NYgfpFWhe5","outputId":"bd95e7b7-a538-4c7c-cfa4-dfedf2619af4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#@title drive'a bağlan\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1746835340646,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"13FXTMs8WxoA","outputId":"0b47f761-0808-4311-d77f-0a3315c98503"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GLASS_09_05_2025\n"]}],"source":["cd /content/drive/MyDrive/GLASS_09_05_2025/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":520,"status":"ok","timestamp":1746835326645,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"jCFHoUNlX0FE","outputId":"9644e297-5345-4ad5-90eb-292b67ccd8dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'GLASS' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/cqylunlun/GLASS"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1746835342735,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"a6PrybgiYH2S","outputId":"93bbe973-79f0-4c92-8a66-aa7f01785b91"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GLASS_09_05_2025/GLASS\n"]}],"source":["cd /content/drive/MyDrive/GLASS_09_05_2025/GLASS/"]},{"cell_type":"code","source":["#@title conda kurulumu\n","# Miniconda'yı indirip kur\n","!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","!chmod +x Miniconda3-latest-Linux-x86_64.sh\n","!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n","\n","# Conda'nın Colab ortamına eklenmesi\n","import sys\n","sys.path.append('/usr/local/lib/python3.10/site-packages')\n","\n","# Conda versiyonunu kontrol et\n","!conda --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"cellView":"form","id":"eUKYycFAeAIW","executionInfo":{"status":"ok","timestamp":1746835389026,"user_tz":-180,"elapsed":24901,"user":{"displayName":"özce yune","userId":"13987651626877442442"}},"outputId":"cfd52e7b-30c0-4fa5-baf4-4e55d3bca302"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-10 00:02:47--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:bf9e, ...\n","Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 155472915 (148M) [application/octet-stream]\n","Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n","\n","Miniconda3-latest-L 100%[===================>] 148.27M  71.7MB/s    in 2.1s    \n","\n","2025-05-10 00:02:50 (71.7 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [155472915/155472915]\n","\n","PREFIX=/usr/local\n","Unpacking payload ...\n","entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n","entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n","\n","Installing base environment...\n","\n","Preparing transaction: ...working... done\n","Executing transaction: ...working... done\n","entry_point.py:256: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n","installation finished.\n","WARNING:\n","    You currently have a PYTHONPATH environment variable set. This may cause\n","    unexpected behavior when running the Python interpreter in Miniconda3.\n","    For best results, please verify that your PYTHONPATH only points to\n","    directories of packages that are compatible with the Python interpreter\n","    in Miniconda3: /usr/local\n","conda 25.3.1\n"]}]},{"cell_type":"code","source":["!conda create -n GLASS python=3.9.15"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"kjYLyB_Ie0mq","executionInfo":{"status":"ok","timestamp":1746835524464,"user_tz":-180,"elapsed":17306,"user":{"displayName":"özce yune","userId":"13987651626877442442"}},"outputId":"a99be634-6ed4-440b-ee6b-ebfc8117a5cb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Channels:\n"," - defaults\n","Platform: linux-64\n","Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n","Solving environment: - \b\bdone\n","\n","## Package Plan ##\n","\n","  environment location: /usr/local/envs/GLASS\n","\n","  added / updated specs:\n","    - python=3.9.15\n","\n","\n","The following packages will be downloaded:\n","\n","    package                    |            build\n","    ---------------------------|-----------------\n","    openssl-1.1.1w             |       h7f8727e_0         3.7 MB\n","    pip-25.1                   |     pyhc872135_2         1.3 MB\n","    python-3.9.15              |       h7a1cb2a_2        25.0 MB\n","    setuptools-78.1.1          |   py39h06a4308_0         1.7 MB\n","    tzdata-2025b               |       h04d1e81_0         116 KB\n","    wheel-0.45.1               |   py39h06a4308_0         114 KB\n","    ------------------------------------------------------------\n","                                           Total:        31.9 MB\n","\n","The following NEW packages will be INSTALLED:\n","\n","  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n","  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n","  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n","  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n","  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n","  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n","  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n","  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n","  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n","  openssl            pkgs/main/linux-64::openssl-1.1.1w-h7f8727e_0 \n","  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n","  python             pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2 \n","  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n","  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py39h06a4308_0 \n","  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n","  tk                 pkgs/main/linux-64::tk-8.6.14-h39e8969_0 \n","  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n","  wheel              pkgs/main/linux-64::wheel-0.45.1-py39h06a4308_0 \n","  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n","  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n","\n","\n","Proceed ([y]/n)? y\n","\n","\n","Downloading and Extracting Packages:\n","python-3.9.15        | 25.0 MB   | :   0% 0/1 [00:00<?, ?it/s]\n","openssl-1.1.1w       | 3.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","setuptools-78.1.1    | 1.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","\n","pip-25.1             | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","tzdata-2025b         | 116 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","wheel-0.45.1         | 114 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","python-3.9.15        | 25.0 MB   | :   7% 0.07368399348377307/1 [00:00<00:01,  1.36s/it]\n","openssl-1.1.1w       | 3.7 MB    | :  67% 0.6699510309917266/1 [00:00<00:00,  6.47it/s]\u001b[A\n","\n","\n","\n","\n","wheel-0.45.1         | 114 KB    | :  70% 0.7034174823973897/1 [00:00<00:00,  7.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","wheel-0.45.1         | 114 KB    | : 100% 1.0/1 [00:00<00:00,  7.01it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","pip-25.1             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  8.50it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","pip-25.1             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  8.50it/s]\u001b[A\u001b[A\u001b[A\n","\n","setuptools-78.1.1    | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  8.38it/s]\u001b[A\u001b[A\n","\n","setuptools-78.1.1    | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  8.38it/s]\u001b[A\u001b[A\n","python-3.9.15        | 25.0 MB   | :  36% 0.3590533580777077/1 [00:00<00:00,  1.97it/s] \n","\n","\n","\n","\n","python-3.9.15        | 25.0 MB   | :  87% 0.8679724656139369/1 [00:00<00:00,  2.34it/s]\n","\n","\n","\n","tzdata-2025b         | 116 KB    | : 100% 1.0/1 [00:00<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","tzdata-2025b         | 116 KB    | : 100% 1.0/1 [00:00<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","python-3.9.15        | 25.0 MB   | : 100% 1.0/1 [00:00<00:00,  2.34it/s]               \n","openssl-1.1.1w       | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  6.47it/s]\u001b[A\n","\n","                                                                        \n","                                                                        \u001b[A\n","\n","                                                                        \u001b[A\u001b[A\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","Preparing transaction: | \b\b/ \b\bdone\n","Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n","Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n","#\n","# To activate this environment, use\n","#\n","#     $ conda activate GLASS\n","#\n","# To deactivate an active environment, use\n","#\n","#     $ conda deactivate\n","\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":311128,"status":"ok","timestamp":1746835880447,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"K41f6W-hXVDA","outputId":"db069c6d-3cc6-4179-e6ce-76a57f5730b7","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting click==8.1.7 (from -r requirements.txt (line 1))\n","  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Collecting cuda-python==11.8.3 (from -r requirements.txt (line 2))\n","  Downloading cuda_python-11.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting imgaug==0.4.0 (from -r requirements.txt (line 3))\n","  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting matplotlib==3.8.2 (from -r requirements.txt (line 4))\n","  Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","Collecting numpy==1.26.3 (from -r requirements.txt (line 5))\n","  Downloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Collecting onnx==1.16.2 (from -r requirements.txt (line 6))\n","  Downloading onnx-1.16.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting onnxruntime-gpu==1.18.1 (from -r requirements.txt (line 7))\n","  Downloading onnxruntime_gpu-1.18.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n","Collecting onnxsim==0.4.36 (from -r requirements.txt (line 8))\n","  Downloading onnxsim-0.4.36-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Collecting opencv-python-headless==4.10.0.84 (from -r requirements.txt (line 9))\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting pandas==1.5.2 (from -r requirements.txt (line 10))\n","  Downloading pandas-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting pillow==10.2.0 (from -r requirements.txt (line 11))\n","  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting scikit-image==0.22.0 (from -r requirements.txt (line 12))\n","  Downloading scikit_image-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Collecting scikit-learn==1.4.0 (from -r requirements.txt (line 13))\n","  Downloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scipy==1.11.4 (from -r requirements.txt (line 14))\n","  Downloading scipy-1.11.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","Collecting tensorboard==2.15.1 (from -r requirements.txt (line 15))\n","  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n","Collecting timm==0.9.12 (from -r requirements.txt (line 16))\n","  Downloading timm-0.9.12-py3-none-any.whl.metadata (60 kB)\n","Collecting torch==2.1.2 (from -r requirements.txt (line 17))\n","  Downloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision==0.16.2 (from -r requirements.txt (line 18))\n","  Downloading torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting tqdm==4.66.1 (from -r requirements.txt (line 19))\n","  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n","Collecting six (from imgaug==0.4.0->-r requirements.txt (line 3))\n","  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting opencv-python (from imgaug==0.4.0->-r requirements.txt (line 3))\n","  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting imageio (from imgaug==0.4.0->-r requirements.txt (line 3))\n","  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n","Collecting Shapely (from imgaug==0.4.0->-r requirements.txt (line 3))\n","  Downloading shapely-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting contourpy>=1.0.1 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n","Collecting cycler>=0.10 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting fonttools>=4.22.0 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading fonttools-4.57.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n","Collecting kiwisolver>=1.3.1 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n","Collecting packaging>=20.0 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting pyparsing>=2.3.1 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n","Collecting python-dateutil>=2.7 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting importlib-resources>=3.2.0 (from matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n","Collecting protobuf>=3.20.2 (from onnx==1.16.2->-r requirements.txt (line 6))\n","  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n","Collecting coloredlogs (from onnxruntime-gpu==1.18.1->-r requirements.txt (line 7))\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Collecting flatbuffers (from onnxruntime-gpu==1.18.1->-r requirements.txt (line 7))\n","  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n","Collecting sympy (from onnxruntime-gpu==1.18.1->-r requirements.txt (line 7))\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting rich (from onnxsim==0.4.36->-r requirements.txt (line 8))\n","  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n","Collecting pytz>=2020.1 (from pandas==1.5.2->-r requirements.txt (line 10))\n","  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting networkx>=2.8 (from scikit-image==0.22.0->-r requirements.txt (line 12))\n","  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n","Collecting tifffile>=2022.8.12 (from scikit-image==0.22.0->-r requirements.txt (line 12))\n","  Downloading tifffile-2024.8.30-py3-none-any.whl.metadata (31 kB)\n","Collecting lazy_loader>=0.3 (from scikit-image==0.22.0->-r requirements.txt (line 12))\n","  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n","Collecting joblib>=1.2.0 (from scikit-learn==1.4.0->-r requirements.txt (line 13))\n","  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n","Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.0->-r requirements.txt (line 13))\n","  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n","Collecting absl-py>=0.4 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n","Collecting grpcio>=1.48.2 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading grpcio-1.71.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting google-auth<3,>=1.6.3 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n","Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n","Collecting markdown>=2.6.8 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n","Collecting protobuf>=3.20.2 (from onnx==1.16.2->-r requirements.txt (line 6))\n","  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n","Collecting requests<3,>=2.21.0 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/envs/GLASS/lib/python3.9/site-packages (from tensorboard==2.15.1->-r requirements.txt (line 15)) (78.1.1)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Collecting pyyaml (from timm==0.9.12->-r requirements.txt (line 16))\n","  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Collecting huggingface-hub (from timm==0.9.12->-r requirements.txt (line 16))\n","  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n","Collecting safetensors (from timm==0.9.12->-r requirements.txt (line 16))\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting filelock (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting typing-extensions (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting jinja2 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting fsspec (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.1.0 (from torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->-r requirements.txt (line 17))\n","  Downloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n","Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n","Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n","Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading charset_normalizer-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n","Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n","Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n","Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib==3.8.2->-r requirements.txt (line 4))\n","  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n","Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n","Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n","Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard==2.15.1->-r requirements.txt (line 15))\n","  Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu==1.18.1->-r requirements.txt (line 7))\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub->timm==0.9.12->-r requirements.txt (line 16))\n","  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n","Collecting markdown-it-py>=2.2.0 (from rich->onnxsim==0.4.36->-r requirements.txt (line 8))\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Collecting pygments<3.0.0,>=2.13.0 (from rich->onnxsim==0.4.36->-r requirements.txt (line 8))\n","  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->onnxsim==0.4.36->-r requirements.txt (line 8))\n","  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime-gpu==1.18.1->-r requirements.txt (line 7))\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Downloading click-8.1.7-py3-none-any.whl (97 kB)\n","Downloading cuda_python-11.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/18.7 MB 3.1 MB/s eta 0:00:00\n","Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 948.0/948.0 kB 55.0 MB/s eta 0:00:00\n","Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 109.7 MB/s eta 0:00:00\n","Downloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 34.6 MB/s eta 0:00:00\n","Downloading onnx-1.16.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 2.3 MB/s eta 0:00:00\n","Downloading onnxruntime_gpu-1.18.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (200.8 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.8/200.8 MB 3.4 MB/s eta 0:00:00\n","Downloading onnxsim-0.4.36-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 8.0 MB/s eta 0:00:00\n","Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.9/49.9 MB 51.8 MB/s eta 0:00:00\n","Downloading pandas-1.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 144.4 MB/s eta 0:00:00\n","Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 158.3 MB/s eta 0:00:00\n","Downloading scikit_image-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 95.7 MB/s eta 0:00:00\n","Downloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 9.9 MB/s eta 0:00:00\n","Downloading scipy-1.11.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.6/36.6 MB 51.0 MB/s eta 0:00:00\n","Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 104.0 MB/s eta 0:00:00\n","Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 112.6 MB/s eta 0:00:00\n","Downloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 13.9 MB/s eta 0:00:00\n","Downloading torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl (6.8 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 117.2 MB/s eta 0:00:00\n","Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n","Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 44.3 MB/s eta 0:00:00\n","Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 116.7 MB/s eta 0:00:00\n","Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 128.8 MB/s eta 0:00:00\n","Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 45.0 MB/s eta 0:00:00\n","Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 17.2 MB/s eta 0:00:00\n","Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 65.0 MB/s eta 0:00:00\n","Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 59.6 MB/s eta 0:00:00\n","Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 33.4 MB/s eta 0:00:00\n","Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 23.6 MB/s eta 0:00:00\n","Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 72.5 MB/s eta 0:00:00\n","Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 MB 61.7 MB/s eta 0:00:00\n","Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n","Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n","Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n","Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n","Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","Downloading charset_normalizer-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","Downloading idna-3.10-py3-none-any.whl (70 kB)\n","Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n","Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 158.1 MB/s eta 0:00:00\n","Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n","Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n","Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n","Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n","Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Downloading fonttools-4.57.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 152.6 MB/s eta 0:00:00\n","Downloading grpcio-1.71.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 162.6 MB/s eta 0:00:00\n","Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n","Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n","Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n","Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 89.1 MB/s eta 0:00:00\n","Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n","Downloading markdown-3.8-py3-none-any.whl (106 kB)\n","Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n","Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 95.0 MB/s eta 0:00:00\n","Downloading packaging-25.0-py3-none-any.whl (66 kB)\n","Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n","Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n","Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n","Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n","Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n","Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n","Downloading tifffile-2024.8.30-py3-none-any.whl (227 kB)\n","Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n","Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n","Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n","Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n","Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n","Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n","Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.6/53.6 MB 59.9 MB/s eta 0:00:00\n","Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 737.4/737.4 kB 44.3 MB/s eta 0:00:00\n","Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n","Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","Downloading nvidia_nvjitlink_cu12-12.9.41-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.7/39.7 MB 76.0 MB/s eta 0:00:00\n","Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 MB 68.9 MB/s eta 0:00:00\n","Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n","Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 78.9 MB/s eta 0:00:00\n","Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","Downloading shapely-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 99.4 MB/s eta 0:00:00\n","Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 155.9 MB/s eta 0:00:00\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 30.5 MB/s eta 0:00:00\n","Installing collected packages: pytz, mpmath, flatbuffers, cuda-python, zipp, urllib3, typing-extensions, tqdm, threadpoolctl, tensorboard-data-server, sympy, six, safetensors, pyyaml, pyparsing, pygments, pyasn1, protobuf, pillow, packaging, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, MarkupSafe, kiwisolver, joblib, idna, humanfriendly, hf-xet, grpcio, fsspec, fonttools, filelock, cycler, click, charset-normalizer, certifi, cachetools, absl-py, werkzeug, triton, tifffile, Shapely, scipy, rsa, requests, python-dateutil, pyasn1-modules, opencv-python-headless, opencv-python, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, lazy_loader, jinja2, importlib-resources, importlib-metadata, imageio, contourpy, coloredlogs, scikit-learn, scikit-image, rich, requests-oauthlib, pandas, onnxruntime-gpu, nvidia-cusolver-cu12, matplotlib, markdown, huggingface-hub, google-auth, torch, onnxsim, imgaug, google-auth-oauthlib, torchvision, tensorboard, timm\n","\n","Successfully installed MarkupSafe-3.0.2 Shapely-2.0.7 absl-py-2.2.2 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.1.7 coloredlogs-15.0.1 contourpy-1.3.0 cuda-python-11.8.3 cycler-0.12.1 filelock-3.18.0 flatbuffers-25.2.10 fonttools-4.57.0 fsspec-2025.3.2 google-auth-2.40.1 google-auth-oauthlib-1.2.2 grpcio-1.71.0 hf-xet-1.1.0 huggingface-hub-0.31.1 humanfriendly-10.0 idna-3.10 imageio-2.37.0 imgaug-0.4.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jinja2-3.1.6 joblib-1.5.0 kiwisolver-1.4.7 lazy_loader-0.4 markdown-3.8 markdown-it-py-3.0.0 matplotlib-3.8.2 mdurl-0.1.2 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.9.41 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 onnx-1.16.2 onnxruntime-gpu-1.18.1 onnxsim-0.4.36 opencv-python-4.11.0.86 opencv-python-headless-4.10.0.84 packaging-25.0 pandas-1.5.2 pillow-10.2.0 protobuf-4.23.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pygments-2.19.1 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 requests-oauthlib-2.0.0 rich-14.0.0 rsa-4.9.1 safetensors-0.5.3 scikit-image-0.22.0 scikit-learn-1.4.0 scipy-1.11.4 six-1.17.0 sympy-1.14.0 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.6.0 tifffile-2024.8.30 timm-0.9.12 torch-2.1.2 torchvision-0.16.2 tqdm-4.66.1 triton-2.1.0 typing-extensions-4.13.2 urllib3-2.4.0 werkzeug-3.1.3 zipp-3.21.0\n","\n"]}],"source":["#@title requirements\n","# gereklilikleri yükle\n","!conda run -n GLASS pip install -r requirements.txt #cuda-python 11.8.2 den 11.8.3 e geçirildi"]},{"cell_type":"code","source":["!conda run -n GLASS pip install openpyxl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"p2ylSBiFjokz","executionInfo":{"status":"ok","timestamp":1746836307967,"user_tz":-180,"elapsed":4652,"user":{"displayName":"özce yune","userId":"13987651626877442442"}},"outputId":"9f8f0bc1-4f8f-474e-e81a-61fef5748113"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openpyxl\n","  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n","Collecting et-xmlfile (from openpyxl)\n","  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n","Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n","Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n","Installing collected packages: et-xmlfile, openpyxl\n","\n","Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n","\n"]}]},{"cell_type":"code","source":["#@title model.py\n","# %load /content/drive/MyDrive/GLASS_09_05_2025/GLASS/model.py\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/model.py\n","\n","import torch\n","import logging\n","\n","LOGGER = logging.getLogger(__name__)\n","\n","def init_weight(m):\n","    if isinstance(m, torch.nn.Linear):\n","        torch.nn.init.xavier_normal_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","    if isinstance(m, torch.nn.BatchNorm1d):\n","        if m.weight is not None: m.weight.data.normal_(1.0, 0.02)\n","        if m.bias is not None: m.bias.data.fill_(0)\n","    elif isinstance(m, torch.nn.Conv2d):\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","\n","\n","class Discriminator(torch.nn.Module):\n","    def __init__(self, in_planes, n_layers=2, hidden=None):\n","        super(Discriminator, self).__init__()\n","        LOGGER.info(f\"Discriminator initialized with in_planes: {in_planes}, n_layers: {n_layers}, hidden: {hidden}\")\n","\n","        _hidden = in_planes if hidden is None else hidden\n","        self.body = torch.nn.Sequential()\n","\n","        current_in_dim = in_planes\n","        for i in range(n_layers - 1):\n","            layer_out_dim = int(current_in_dim // 1.5) if hidden is None else _hidden # hidden None ise küçülür, değilse sabit\n","\n","            self.body.add_module('block%d' % (i + 1),\n","                                  torch.nn.Sequential(\n","                                      torch.nn.Linear(current_in_dim, layer_out_dim),\n","                                      torch.nn.BatchNorm1d(layer_out_dim),\n","                                      torch.nn.LeakyReLU(0.2)\n","                                  ))\n","            current_in_dim = layer_out_dim\n","\n","        self.tail = torch.nn.Sequential(\n","            torch.nn.Linear(current_in_dim, 1, bias=False),\n","            torch.nn.Sigmoid()\n","        )\n","        self.apply(init_weight)\n","\n","    def forward(self, x):\n","        LOGGER.info(f\"Discriminator input shape: {x.shape if isinstance(x, torch.Tensor) else type(x)}\")\n","        x_body = self.body(x)\n","        LOGGER.info(f\"Discriminator after body shape: {x_body.shape if isinstance(x_body, torch.Tensor) else type(x_body)}\")\n","        x_tail = self.tail(x_body)\n","        LOGGER.info(f\"Discriminator final output shape: {x_tail.shape if isinstance(x_tail, torch.Tensor) else type(x_tail)}\")\n","        return x_tail\n","\n","\n","class Projection(torch.nn.Module):\n","    def __init__(self, in_planes, out_planes=None, n_layers=1, layer_type=0):\n","        super(Projection, self).__init__()\n","        LOGGER.info(f\"Projection initialized with in_planes: {in_planes}, out_planes: {out_planes}, n_layers: {n_layers}, layer_type: {layer_type}\")\n","\n","        if out_planes is None:\n","            out_planes = in_planes\n","        self.layers = torch.nn.Sequential()\n","\n","        current_in_dim = in_planes\n","        for i in range(n_layers):\n","            current_out_dim = out_planes\n","\n","            self.layers.add_module(f\"{i}fc\", torch.nn.Linear(current_in_dim, current_out_dim))\n","            if i < n_layers - 1:\n","                if layer_type == 1:\n","                     self.layers.add_module(f\"{i}relu\", torch.nn.ReLU())\n","                elif layer_type > 1 :\n","                     self.layers.add_module(f\"{i}lrelu\", torch.nn.LeakyReLU(.2))\n","            current_in_dim = current_out_dim\n","\n","        self.apply(init_weight)\n","\n","    def forward(self, x):\n","        LOGGER.info(f\"Projection input shape (actual x.shape): {x.shape if isinstance(x, torch.Tensor) else type(x)}\")\n","        processed_x = self.layers(x)\n","        LOGGER.info(f\"Projection output shape (actual processed_x.shape): {processed_x.shape if isinstance(processed_x, torch.Tensor) else type(processed_x)}\")\n","        return processed_x\n","\n","\n","class PatchMaker:\n","    def __init__(self, patchsize, top_k=0, stride=None):\n","        self.patchsize = patchsize\n","        self.stride = stride if stride is not None else 1\n","        self.top_k = top_k\n","\n","    def patchify(self, features, return_spatial_info=False):\n","        padding = int((self.patchsize - 1) / 2)\n","        unfolder = torch.nn.Unfold(\n","            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n","        )\n","        unfolded_features = unfolder(features)\n","\n","        spatial_dims_out = []\n","        for s_in in features.shape[-2:]:\n","            n_patches = (s_in + 2 * padding - (self.patchsize - 1) - 1) / self.stride + 1\n","            spatial_dims_out.append(int(n_patches))\n","\n","        # Orijinal koddaki reshape ve permute mantığına sadık kalalım:\n","        # features (B,C,H,W)\n","        # unfolded_features (B, C*patchsize*patchsize, L) where L = N_patches_H * N_patches_W\n","        # Hedef: (B, L, C, patchsize, patchsize)\n","        try:\n","            unfolded_reshaped = unfolded_features.view( # unfolded_features -> unfolded_reshaped\n","                features.shape[0], features.shape[1], self.patchsize, self.patchsize, -1 # (B, C, ps, ps, L)\n","            )\n","            unfolded_permuted = unfolded_reshaped.permute(0, 4, 1, 2, 3) # (B, L, C, ps, ps)\n","        except Exception as e:\n","            LOGGER.error(f\"Error in PatchMaker.patchify reshape/permute: {e}. Unfolded shape: {unfolded_features.shape}, features shape: {features.shape}\")\n","            # Hata durumunda orijinal unfolded_features'ı döndür, bu downstream'de sorun yaratabilir.\n","            unfolded_permuted = unfolded_features\n","\n","\n","        if return_spatial_info:\n","            return unfolded_permuted, spatial_dims_out\n","        return unfolded_permuted\n","\n","    def unpatch_scores(self, x, batchsize):\n","        # x: (ToplamPatchSayisi, SkorBoyutu) örn: (B * N_H * N_W, 1)\n","        # Hedef: (B, N_H * N_W, SkorBoyutu) veya (B, N_H, N_W) eğer SkorBoyutu=1 ve squeeze edildiyse.\n","        # Orijinal _predict bunu (B, N_H, N_W) olarak reshape ediyordu.\n","        # Bu fonksiyonun (B, N_H*N_W, SkorBoyutu) döndürmesi daha mantıklı.\n","        if x.shape[0] > 0 and batchsize > 0 and x.shape[0] % batchsize == 0:\n","             num_patches_per_image = x.shape[0] // batchsize\n","             return x.view(batchsize, num_patches_per_image, *x.shape[1:])\n","        else:\n","             LOGGER.warning(f\"PatchMaker.unpatch_scores: Cannot reliably unpatch. Input shape {x.shape}, batchsize {batchsize}. Returning as is or reshaped with -1.\")\n","             # Orijinaldeki gibi -1 ile reshape etmeyi deneyelim, ama bu B, H_patch, W_patch bilgisini gerektirir.\n","             # Şimdilik, eğer batchsize'a bölünmüyorsa, sadece ilk boyutu batchsize yapıp kalanı -1 yapalım.\n","             try:\n","                 return x.reshape(batchsize, -1, *x.shape[1:])\n","             except RuntimeError: # Eğer reshape mümkün değilse (örn. x boşsa)\n","                 return x\n","\n","\n","    def score(self, x): # x: (B, N_patches_per_image, D_score) veya (B, N_patches_per_image)\n","        if x.ndim == 3 and x.shape[-1] == 1:\n","            x = x.squeeze(-1)\n","\n","        if x.ndim == 2: # (B, N_patch) ise\n","            x_scores, _ = torch.max(x, dim=1) # .values -> PyTorch eski versiyonlarda, yeni versiyonlarda _ ile ikinci değeri al\n","        elif x.ndim == 1 and x.shape[0] > 0 : # Eğer tek bir batch item için (N_patch,) geldiyse\n","            x_scores, _ = torch.max(x, dim=0, keepdim=True) # (1,) şeklinde döner\n","        else:\n","            LOGGER.error(f\"PatchMaker.score input x has unexpected shape {x.shape}. Expected (B,N) or (B,N,1).\")\n","            x_scores = torch.zeros(x.shape[0] if x.ndim > 0 and x.shape[0] > 0 else 1, device=x.device)\n","\n","        return x_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"23y_hpQzp-yf","executionInfo":{"status":"ok","timestamp":1746838890813,"user_tz":-180,"elapsed":41,"user":{"displayName":"özce yune","userId":"13987651626877442442"}},"outputId":"7ff19ae2-4587-4e4c-8f50-30cb44c9cb57"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/model.py\n"]}]},{"cell_type":"code","execution_count":42,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1746838810842,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"ZdNLbY52LLi4","outputId":"25915d62-0e6d-4b04-9df6-160ebc524387"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/common.py\n"]}],"source":["#@title common.py\n","# %load /content/drive/MyDrive/GLASS_09_05_2025/GLASS/common.py\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/common.py\n","\n","import copy\n","import numpy as np\n","import scipy.ndimage as ndimage\n","import torch\n","import torch.nn.functional as F\n","import logging\n","\n","LOGGER = logging.getLogger(__name__)\n","\n","class Preprocessing(torch.nn.Module):\n","    def __init__(self, input_dims, output_dim):\n","        super(Preprocessing, self).__init__()\n","        self.input_dims = input_dims\n","        self.output_dim = output_dim\n","\n","        self.preprocessing_modules = torch.nn.ModuleList()\n","        for _ in input_dims:\n","            module = MeanMapper(output_dim)\n","            self.preprocessing_modules.append(module)\n","\n","    def forward(self, features):\n","        _features = []\n","        for module, feature in zip(self.preprocessing_modules, features):\n","            _features.append(module(feature))\n","        return torch.stack(_features, dim=1)\n","\n","\n","class MeanMapper(torch.nn.Module):\n","    def __init__(self, preprocessing_dim):\n","        super(MeanMapper, self).__init__()\n","        self.preprocessing_dim = preprocessing_dim\n","\n","    def forward(self, features):\n","        features = features.reshape(len(features), 1, -1)\n","        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n","\n","\n","class Aggregator(torch.nn.Module):\n","    def __init__(self, target_dim):\n","        super(Aggregator, self).__init__()\n","        self.target_dim = target_dim\n","        LOGGER.info(f\"Aggregator initialized with target_dim: {self.target_dim}\")\n","\n","    def forward(self, features): # Beklenen girdi: (NumPatches, NumFeatureLevels, EmbeddingDimPerLevel)\n","        \"\"\"Returns reshaped and average pooled features.\"\"\"\n","        LOGGER.info(f\"Aggregator input features shape: {features.shape if isinstance(features, torch.Tensor) else type(features)}\")\n","\n","        reshaped_features = features.reshape(len(features), 1, -1)\n","        LOGGER.info(f\"Aggregator after reshape to (NumPatches, 1, -1): {reshaped_features.shape}\")\n","\n","        pooled_features = F.adaptive_avg_pool1d(reshaped_features, self.target_dim)\n","        LOGGER.info(f\"Aggregator after adaptive_avg_pool1d (target_dim={self.target_dim}): {pooled_features.shape}\")\n","\n","        output_features = pooled_features.reshape(len(features), -1)\n","        LOGGER.info(f\"Aggregator final output shape: {output_features.shape}\")\n","        return output_features\n","\n","\n","class RescaleSegmentor:\n","    def __init__(self, device, target_size=288):\n","        self.device = device\n","        self.target_size = target_size\n","        self.smoothing = 4\n","        LOGGER.info(f\"RescaleSegmentor: Initialized with target_size: {self.target_size}, type: {type(self.target_size)}\")\n","\n","    def convert_to_segmentation(self, patch_scores):\n","        with torch.no_grad():\n","            if isinstance(patch_scores, np.ndarray):\n","                patch_scores_tensor = torch.from_numpy(patch_scores).float().to(self.device)\n","            else:\n","                patch_scores_tensor = patch_scores.to(self.device).float()\n","\n","            _scores = patch_scores_tensor\n","\n","            # LOGGER.info(f\"RescaleSegmentor: Input patch_scores shape to convert_to_segmentation: {_scores.shape}\")\n","\n","            if _scores.ndim == 3:\n","                _scores = _scores.unsqueeze(1)\n","            elif _scores.ndim == 2:\n","                if _scores.shape[0] > 1 and _scores.shape[1] > 1 :\n","                    _scores = _scores.unsqueeze(0).unsqueeze(0)\n","                else:\n","                    LOGGER.error(f\"RescaleSegmentor: Received 2D input with ambiguous shape: {_scores.shape}.\")\n","                    h_dummy, w_dummy = (self.target_size, self.target_size) if isinstance(self.target_size, int) else self.target_size\n","                    return [np.zeros((h_dummy, w_dummy))] * (patch_scores_tensor.shape[0] if patch_scores_tensor.ndim > 0 and patch_scores_tensor.shape[0] > 0 else 1)\n","\n","            elif _scores.ndim == 4 and _scores.shape[1] != 1:\n","                 LOGGER.warning(f\"RescaleSegmentor: Input has {_scores.shape[1]} channels, expected 1. Using only the first channel.\")\n","                 _scores = _scores[:, 0:1, :, :]\n","            elif _scores.ndim != 4 :\n","                 LOGGER.error(f\"RescaleSegmentor: Unexpected input scores shape before interpolate: {_scores.shape}.\")\n","                 batch_dim_for_dummy = 1\n","                 if _scores.ndim > 0 : batch_dim_for_dummy = _scores.shape[0] if _scores.shape[0] > 0 else 1\n","                 target_h, target_w = (self.target_size, self.target_size) if isinstance(self.target_size, int) else self.target_size\n","                 _scores = torch.zeros((batch_dim_for_dummy, 1, target_h, target_w), device=self.device)\n","\n","            current_target_size = (self.target_size, self.target_size) if isinstance(self.target_size, int) else self.target_size\n","            # LOGGER.info(f\"RescaleSegmentor: Shape before F.interpolate: {_scores.shape}, target_size used: {current_target_size}\")\n","\n","            interpolated_scores = F.interpolate(\n","                _scores, size=current_target_size, mode=\"bilinear\", align_corners=False\n","            )\n","            # LOGGER.info(f\"RescaleSegmentor: Shape AFTER F.interpolate: {interpolated_scores.shape}\")\n","\n","            if interpolated_scores.ndim == 4 and interpolated_scores.shape[1] == 1:\n","                squeezed_scores = interpolated_scores.squeeze(1)\n","            elif interpolated_scores.ndim == 3 :\n","                squeezed_scores = interpolated_scores\n","            else:\n","                LOGGER.error(f\"RescaleSegmentor: Unexpected shape after interpolate for squeeze: {interpolated_scores.shape}. Expected (B, 1, H, W).\")\n","                batch_dim_for_dummy = interpolated_scores.shape[0] if interpolated_scores.ndim > 0 and interpolated_scores.shape[0] > 0 else 1\n","                target_h, target_w = current_target_size\n","                squeezed_scores = torch.zeros((batch_dim_for_dummy, target_h, target_w), device=self.device)\n","\n","            # LOGGER.info(f\"RescaleSegmentor: Shape AFTER squeeze(1): {squeezed_scores.shape}\")\n","            numpy_scores = squeezed_scores.cpu().numpy()\n","\n","        if numpy_scores.ndim == 3:\n","            return [ndimage.gaussian_filter(score_item, sigma=self.smoothing) for score_item in numpy_scores]\n","        elif numpy_scores.ndim == 2:\n","            return [ndimage.gaussian_filter(numpy_scores, sigma=self.smoothing)]\n","        else:\n","            LOGGER.error(f\"RescaleSegmentor: Output numpy_scores has unexpected ndim for gaussian_filter: {numpy_scores.ndim}, shape: {numpy_scores.shape}. Returning empty list.\")\n","            return []\n","\n","\n","class NetworkFeatureAggregator(torch.nn.Module):\n","    def __init__(self, backbone, layers_to_extract_from, device, train_backbone=False):\n","        super(NetworkFeatureAggregator, self).__init__()\n","        self.layers_to_extract_from = layers_to_extract_from\n","        self.backbone = backbone\n","        self.device = device\n","        self.train_backbone = train_backbone\n","        if not hasattr(backbone, \"hook_handles\"):\n","            self.backbone.hook_handles = []\n","        for handle in self.backbone.hook_handles:\n","            handle.remove()\n","        self.outputs = {}\n","        for extract_layer in layers_to_extract_from:\n","            self.register_hook(extract_layer)\n","        self.to(self.device)\n","\n","    def forward(self, images, eval=True):\n","        self.outputs.clear()\n","        if self.train_backbone and not eval:\n","            self.backbone(images)\n","        else:\n","            with torch.no_grad():\n","                try:\n","                    _ = self.backbone(images)\n","                except LastLayerToExtractReachedException:\n","                    pass\n","        return self.outputs\n","\n","    def feature_dimensions(self, input_shape):\n","        _input = torch.ones([1] + list(input_shape)).to(self.device)\n","        _output = self(_input, eval=True)\n","        # _output bir sözlük, layers_to_extract_from içindeki her katman için bir anahtar içerir\n","        # Her bir _output[layer]'ın (B, C, H, W) veya (B, L, C) gibi bir şekli vardır.\n","        # Genellikle kanal sayısı (C) istenir, bu da shape[1]'dir.\n","        dims = []\n","        for layer in self.layers_to_extract_from:\n","            if layer in _output:\n","                dims.append(_output[layer].shape[1])\n","            else:\n","                LOGGER.error(f\"Layer {layer} not found in NetworkFeatureAggregator outputs during feature_dimensions calculation.\")\n","                dims.append(0) # Hata durumunda varsayılan boyut\n","        return dims\n","\n","    def find_module(self, model, module_name): # Kullanıcının orijinal metodu\n","        for name, module in model.named_modules():\n","            if name == module_name:\n","                return module\n","            elif '.' in module_name:\n","                father, child = module_name.split('.', 1)\n","                if name == father:\n","                    return self.find_module(module, child)\n","        return None\n","\n","    def register_hook(self, layer_name):\n","        module = self.find_module(self.backbone, layer_name)\n","        if module is not None:\n","            forward_hook = ForwardHook(self.outputs, layer_name, self.layers_to_extract_from[-1])\n","            if isinstance(module, torch.nn.Sequential) and len(module) > 0:\n","                hook = module[-1].register_forward_hook(forward_hook)\n","            else:\n","                hook = module.register_forward_hook(forward_hook)\n","            self.backbone.hook_handles.append(hook)\n","        else:\n","            available_modules = [name for name, _ in self.backbone.named_modules()]\n","            LOGGER.error(f\"Module {layer_name} not found in the model. Available top-level modules: {list(self.backbone.named_children())}\")\n","            LOGGER.debug(f\"All available named modules: {available_modules}\")\n","            raise ValueError(f\"Module {layer_name} not found in the model\")\n","\n","\n","class ForwardHook:\n","    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n","        self.hook_dict = hook_dict\n","        self.layer_name = layer_name\n","        self.raise_exception_to_break = copy.deepcopy(\n","            layer_name == last_layer_to_extract\n","        )\n","\n","    def __call__(self, module, input, output):\n","        self.hook_dict[self.layer_name] = output\n","        if self.raise_exception_to_break:\n","            raise LastLayerToExtractReachedException()\n","        return None\n","\n","\n","class LastLayerToExtractReachedException(Exception):\n","    pass"]},{"cell_type":"code","execution_count":36,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1746838058593,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"BWPmVrMA0SVk","outputId":"42ead6dc-9121-4f6f-e7d4-fb57813e4cc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/metrics.py\n"]}],"source":["#@title metrics.py\n","# %load metrics.py\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/metrics.py\n","\n","from sklearn import metrics\n","from skimage import measure\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","# Kullanıcının sağladığı orijinal compute_best_pr_re fonksiyonu (Dokunulmadı)\n","def compute_best_pr_re(anomaly_ground_truth_labels, anomaly_prediction_weights):\n","    \"\"\"\n","    Computes the best precision, recall and threshold for a given set of\n","    anomaly ground truth labels and anomaly prediction weights.\n","    \"\"\"\n","    precision, recall, thresholds = metrics.precision_recall_curve(anomaly_ground_truth_labels, anomaly_prediction_weights)\n","    # F1 skorlarını hesapla (precision ve recall sıfır ise NaN/hata olabilir, 1e-10 ekleyerek önle)\n","    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n","    f1_scores = np.nan_to_num(f1_scores, nan=0.0) # NaN olan F1 skorlarını 0 yap\n","\n","    # En iyi F1 skorunun indeksini bul\n","    # Eğer f1_scores boşsa veya hepsi 0 ise argmax sorun çıkarabilir.\n","    if len(f1_scores) == 0:\n","        # Bu durum precision/recall boşsa veya tek elemanlıysa olabilir.\n","        # Varsayılan değerler döndürebiliriz.\n","        # print(\"Warning: f1_scores is empty in compute_best_pr_re.\")\n","        return thresholds[0] if len(thresholds) > 0 else 0.5, 0.0, 0.0 # Örnek varsayılanlar\n","\n","    best_f1_idx = np.argmax(f1_scores)\n","\n","    # En iyi F1'e karşılık gelen eşik, precision ve recall değerlerini bul\n","    # thresholds dizisi, precision/recall dizilerinden bir eleman kısa olabilir.\n","    # thresholds[i], precision[i+1] ve recall[i+1]'e karşılık gelir.\n","    # Bu yüzden, best_f1_idx'e göre doğru threshold'u seçmeliyiz.\n","    if best_f1_idx == 0 and len(precision) > len(thresholds): # İlk P/R çifti için (genellikle R=0, P=1)\n","        # Bu durumda, thresholds'un ilk elemanını (en düşük skorlara karşılık gelen)\n","        # veya skorların maksimumunu eşik olarak almak mantıklı olabilir.\n","        # Ya da, F1'i precision[1:] ve recall[1:] için hesaplayıp, argmax'ı thresholds için kullanabiliriz.\n","        # Şimdilik, eğer F1 en başta max ise, thresholds'un ilk elemanını alalım (eğer varsa).\n","        best_threshold = thresholds[0] if len(thresholds) > 0 else (anomaly_prediction_weights.min() if len(anomaly_prediction_weights) > 0 else 0.5)\n","    elif best_f1_idx > 0 and best_f1_idx <= len(thresholds) : # Normal durum\n","        best_threshold = thresholds[best_f1_idx - 1] # F1'i P,R'nin tümü için hesapladıysak ve P/R, T'den 1 uzunsa\n","    elif best_f1_idx > len(thresholds) and len(thresholds) > 0: # Son P/R çifti için (genellikle R=1, P=düşük)\n","        best_threshold = thresholds[-1]\n","    elif len(thresholds) > 0 : # Genel bir fallback\n","        best_threshold = thresholds[min(best_f1_idx, len(thresholds)-1)]\n","    else: # thresholds boşsa\n","        best_threshold = anomaly_prediction_weights.mean() if len(anomaly_prediction_weights) > 0 else 0.5\n","\n","\n","    best_precision = precision[best_f1_idx]\n","    best_recall = recall[best_f1_idx]\n","    print(f\"Output from compute_best_pr_re -> Best Threshold: {best_threshold:.4f}, Precision: {best_precision:.4f}, Recall: {best_recall:.4f}\") # Orijinal print ifadesi\n","    return best_threshold, best_precision, best_recall\n","\n","\n","def compute_imagewise_retrieval_metrics(anomaly_prediction_weights, anomaly_ground_truth_labels, path='training'):\n","    \"\"\"\n","    Computes retrieval statistics (AUROC, AP, max I-F1, best threshold for I-F1).\n","    \"\"\"\n","    auroc = metrics.roc_auc_score(anomaly_ground_truth_labels, anomaly_prediction_weights)\n","    ap = 0. if path == 'training' else metrics.average_precision_score(anomaly_ground_truth_labels, anomaly_prediction_weights)\n","\n","    # Max I-F1 ve eşik değerini hesapla\n","    precision, recall, thresholds = metrics.precision_recall_curve(\n","        anomaly_ground_truth_labels, anomaly_prediction_weights\n","    )\n","    f1_scores = (2 * precision * recall) / (precision + recall + 1e-10)\n","    f1_scores = np.nan_to_num(f1_scores, nan=0.0)\n","\n","    if len(f1_scores) == 0:\n","        max_f1 = 0.0\n","        best_f1_thresh = 0.5 # Varsayılan\n","    else:\n","        best_f1_idx = np.argmax(f1_scores)\n","        max_f1 = f1_scores[best_f1_idx]\n","        # Eşik değeri ataması için benzer mantık\n","        if best_f1_idx == 0 and len(precision) > len(thresholds):\n","             best_f1_thresh = thresholds[0] if len(thresholds) > 0 else (anomaly_prediction_weights.min() if len(anomaly_prediction_weights) > 0 else 0.5)\n","        elif best_f1_idx > 0 and best_f1_idx <= len(thresholds) :\n","            best_f1_thresh = thresholds[best_f1_idx - 1]\n","        elif best_f1_idx > len(thresholds) and len(thresholds) > 0:\n","            best_f1_thresh = thresholds[-1]\n","        elif len(thresholds) > 0 :\n","             best_f1_thresh = thresholds[min(best_f1_idx, len(thresholds)-1)]\n","        else:\n","            best_f1_thresh = anomaly_prediction_weights.mean() if len(anomaly_prediction_weights) > 0 else 0.5\n","\n","    return {\"auroc\": auroc, \"ap\": ap, \"max_f1\": max_f1, \"f1_threshold\": best_f1_thresh}\n","\n","# --- YENİ EKLENEN FONKSİYONLAR (Piksel F1 ve IoU için) ---\n","def compute_pixel_f1(ground_truth_flat_masks, anomaly_flat_segmentations_heatmap, threshold=0.5):\n","    \"\"\" Computes pixel-wise F1 score after thresholding heatmap. \"\"\"\n","    predictions_binary = (anomaly_flat_segmentations_heatmap >= threshold).astype(np.uint8)\n","    # ground_truth_flat_masks'ın zaten binary (0/1) olduğunu varsayıyoruz.\n","    if np.sum(ground_truth_flat_masks) == 0 and np.sum(predictions_binary) == 0: # İkisi de boşsa (hiç anomali yok ve bulunmadı)\n","        return 1.0\n","    return metrics.f1_score(ground_truth_flat_masks, predictions_binary, average='binary', zero_division=0)\n","\n","def compute_pixel_iou(ground_truth_flat_masks, anomaly_flat_segmentations_heatmap, threshold=0.5):\n","    \"\"\" Computes pixel-wise IoU (Jaccard) score after thresholding heatmap. \"\"\"\n","    predictions_binary = (anomaly_flat_segmentations_heatmap >= threshold).astype(np.uint8)\n","    # ground_truth_flat_masks'ın zaten binary (0/1) olduğunu varsayıyoruz.\n","    if np.sum(ground_truth_flat_masks) == 0 and np.sum(predictions_binary) == 0: # İkisi de boşsa\n","        return 1.0\n","    return metrics.jaccard_score(ground_truth_flat_masks, predictions_binary, average='binary', zero_division=0)\n","# --- YENİ EKLENEN FONKSİYONLAR SONU ---\n","\n","def compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks, path='training', segmentation_threshold=0.5): # path='train' -> path='training' yaptım, imagewise ile tutarlı olsun\n","    \"\"\"\n","    Computes pixel-wise statistics (AUROC, AP, F1, IoU) for anomaly segmentations\n","    and ground truth segmentation masks.\n","    anomaly_segmentations: predicted heatmaps\n","    ground_truth_masks: binary ground truth masks\n","    segmentation_threshold: threshold to binarize heatmaps for F1 and IoU\n","    \"\"\"\n","    if isinstance(anomaly_segmentations, list):\n","        anomaly_segmentations = np.stack(anomaly_segmentations)\n","    if isinstance(ground_truth_masks, list):\n","        ground_truth_masks = np.stack(ground_truth_masks)\n","\n","    # Ground truth maskelerin kesinlikle binary (0 veya 1 int) olduğundan emin olalım\n","    flat_ground_truth_masks = (ground_truth_masks.ravel() > 0.5).astype(np.uint8)\n","    flat_anomaly_segmentations_heatmap = anomaly_segmentations.ravel() # Bunlar olasılık/heatmap değerleri\n","\n","    auroc = metrics.roc_auc_score(flat_ground_truth_masks, flat_anomaly_segmentations_heatmap)\n","    # path == 'training' ise ap = 0.0 (orijinal mantık)\n","    ap = 0. if path == 'training' else metrics.average_precision_score(flat_ground_truth_masks, flat_anomaly_segmentations_heatmap)\n","\n","    # Yeni eklenen Pixel F1 ve IoU hesaplamaları\n","    pixel_f1 = compute_pixel_f1(flat_ground_truth_masks, flat_anomaly_segmentations_heatmap, threshold=segmentation_threshold)\n","    pixel_iou = compute_pixel_iou(flat_ground_truth_masks, flat_anomaly_segmentations_heatmap, threshold=segmentation_threshold)\n","\n","    return {\"auroc\": auroc, \"ap\": ap, \"f1\": pixel_f1, \"iou\": pixel_iou}\n","\n","\n","def compute_pro(masks, amaps, num_th=200): # Bu fonksiyon olduğu gibi kalabilir, iç mantığına dokunmadım\n","    # ... (Önceki mesajda verdiğim compute_pro fonksiyonunun tamamı buraya gelecek,\n","    #      max_th == min_th durumu, threshold_range ve FPR normalizasyonundaki\n","    #      küçük düzeltmeleri içeren haliyle.)\n","    # compute_pro fonksiyonunun tam içeriğini bir önceki yanıttan kopyalayıp buraya yapıştırabilirsiniz.\n","    # Aşağıya o içeriği tekrar ekliyorum:\n","\n","    df = pd.DataFrame([], columns=[\"pro\", \"fpr\", \"threshold\"])\n","    # amaps boşsa veya maskelerle uyuşmuyorsa erken çıkış\n","    if amaps.size == 0 or masks.size == 0 or amaps.shape[0] != masks.shape[0]:\n","        return 0.0\n","\n","    binary_amaps = np.zeros_like(amaps, dtype=bool)\n","\n","    min_th = amaps.min()\n","    max_th = amaps.max()\n","    delta = 0.1 # Varsayılan delta, eğer max_th == min_th ise\n","\n","    if max_th > min_th and num_th > 0:\n","        delta = (max_th - min_th) / num_th\n","    elif max_th == min_th: # Eğer tüm değerler aynıysa, tek bir eşik (değerin kendisi) ile değerlendirme.\n","        # Bu durumda PRO ya 0 ya da 1 olacak (maskeye bağlı).\n","        # Ya da bu durumu özel ele alıp 0 veya 1 döndür.\n","        # Şimdilik, tek bir eşik değeri ile devam edelim.\n","        num_th = 1 # Tek iterasyon\n","        # delta'nın bir önemi kalmıyor ama sıfır olmasın diye ayarlanabilir.\n","        # Ya da threshold_range'i doğrudan [min_th] olarak ayarla.\n","    elif num_th == 0: # num_th 0 ise\n","        return 0.0\n","\n","\n","    k = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n","\n","    if max_th == min_th:\n","        threshold_range = np.array([min_th])\n","    else:\n","        threshold_range = np.arange(min_th, max_th + delta, delta)\n","        if len(threshold_range) == 0 : # Aralık çok küçükse ve arange boş dönerse\n","            threshold_range = np.array([min_th,max_th])\n","\n","\n","    for th in threshold_range:\n","        binary_amaps[amaps <= th] = 0\n","        binary_amaps[amaps > th] = 1\n","\n","        pros_for_this_threshold = [] # pros -> pros_for_this_threshold (isim çakışması olmasın)\n","        for binary_amap_single, mask_single in zip(binary_amaps, masks): # binary_amap, mask -> *_single\n","            # Ensure mask_single is binary {0,1} for regionprops\n","            binary_mask_for_props = (mask_single > 0.5).astype(np.uint8)\n","\n","            binary_amap_dilated = cv2.dilate(binary_amap_single.astype(np.uint8), k)\n","\n","            labeled_mask = measure.label(binary_mask_for_props)\n","            if labeled_mask.max() == 0: # Maskede hiç bölge yoksa\n","                # Eğer hem maske hem de tahmin edilen anomali bölgesi yoksa, bu \"doğru\" bir durumdur.\n","                # Ancak PRO tanımı gereği bölge üzerinden hesaplanır.\n","                # Bu durumu nasıl ele alacağınız önemli. 0 eklemek ortalamayı düşürür.\n","                # Belki de bu imajı PRO hesaplamasına dahil etmemek daha doğru olur.\n","                # Şimdilik, eğer maskede bölge yoksa, o imaj için PRO'yu 0 kabul edelim (veya skip).\n","                # Ya da, eğer binary_amap_dilated da boşsa 1.0 eklenebilir.\n","                # En iyisi, eğer hiç bölge yoksa, bu imaj için pros listesine bir şey eklememek.\n","                continue # Bu imaj için bölge yok, bir sonraki imaja geç.\n","\n","            for region in measure.regionprops(labeled_mask):\n","                if region.area == 0: continue # Alanı 0 olan bölgeleri atla\n","                axes0_ids = region.coords[:, 0]\n","                axes1_ids = region.coords[:, 1]\n","                tp_pixels = binary_amap_dilated[axes0_ids, axes1_ids].sum()\n","                pros_for_this_threshold.append(tp_pixels / region.area)\n","\n","        if not pros_for_this_threshold: # Eğer listede hiç PRO değeri yoksa (örn. tüm maskeler boştu)\n","            mean_pros = 0.0 # Veya NaN ve sonra handle et. Ya da bu eşiği atla.\n","        else:\n","            mean_pros = np.mean(pros_for_this_threshold)\n","\n","        # masks'ın binary olduğundan emin ol (0/1)\n","        binary_masks_for_fpr = (masks > 0.5).astype(np.uint8)\n","        inverse_masks = 1 - binary_masks_for_fpr\n","        fp_pixels = np.logical_and(inverse_masks, binary_amaps).sum() # binary_amaps tüm batch için tek\n","\n","        if inverse_masks.sum() == 0: # Tüm pikseller foreground ise (anomali ise)\n","            fpr = 0.0 if fp_pixels == 0 else 1.0 # Tüm pikseller anomali ise ve FP varsa FPR=1\n","        else:\n","            fpr = fp_pixels / inverse_masks.sum()\n","\n","        df = pd.concat([df, pd.DataFrame({\"pro\": mean_pros, \"fpr\": fpr, \"threshold\": th}, index=[0])], ignore_index=True)\n","\n","    if df.empty: # Eğer df hiç doldurulmadıysa (örn. threshold_range boştu)\n","        return 0.0\n","\n","    df_filtered = df[df[\"fpr\"] < 0.3].copy()\n","    if df_filtered.empty:\n","        # Eğer FPR < 0.3 olan hiç nokta yoksa, tüm df üzerinden hesapla veya 0 döndür.\n","        # Belki de tüm df'i kullanmak daha iyi bir fallback olur.\n","        # Veya FPR filtresini uygulamadan önceki sonuca bak.\n","        # Şimdilik 0 döndürelim.\n","        # Alternatif olarak, filtresiz df üzerinden auc hesaplanabilir.\n","        # Veya df'deki en düşük fpr'li pro değeri döndürülebilir.\n","        # En basit çözüm 0.0.\n","        if not df.empty: # Filtresiz df boş değilse, onun üzerinden bir şeyler deneyebiliriz.\n","            # Örneğin, tüm aralık için AUC:\n","            # df_filtered = df.copy() # Tüm df'i kullan\n","            # if df_filtered.empty: return 0.0 # Bu olmamalı artık\n","            # else: # devam et... (ama bu mantık döngüye sokar)\n","            # Şimdilik, eğer filtre sonucu boşsa 0.0 döndür.\n","            return 0.0\n","\n","\n","    min_fpr = df_filtered[\"fpr\"].min()\n","    max_fpr = df_filtered[\"fpr\"].max()\n","\n","    if max_fpr == min_fpr :\n","        df_filtered.loc[:, \"fpr_normalized\"] = 0.0\n","    else:\n","        df_filtered.loc[:, \"fpr_normalized\"] = (df_filtered[\"fpr\"] - min_fpr) / (max_fpr - min_fpr + 1e-10)\n","\n","    # Ensure fpr_normalized is sorted for AUC calculation if not already.\n","    # metrics.auc expects x to be sorted. df_filtered'ı fpr_normalized'e göre sırala.\n","    df_filtered = df_filtered.sort_values(by=\"fpr_normalized\")\n","\n","    pro_auc = metrics.auc(df_filtered[\"fpr_normalized\"], df_filtered[\"pro\"])\n","    return pro_auc"]},{"cell_type":"code","execution_count":53,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1746839343012,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"AWAQ5MnFzzLx","outputId":"8a3eebb4-7a2e-42b1-c42f-00e37bd2c755"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/glass.py\n"]}],"source":["#@title glass.py\n","# %load glass.py\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/glass.py\n","\n","from loss import FocalLoss\n","from collections import OrderedDict\n","from torchvision import transforms\n","from torch.utils.tensorboard import SummaryWriter\n","from model import Discriminator, Projection, PatchMaker\n","\n","import numpy as np\n","import pandas as pd\n","import torch.nn.functional as F\n","\n","import logging\n","import os\n","import math\n","import torch\n","import tqdm\n","import common\n","import metrics\n","import cv2\n","import utils\n","import glob\n","import shutil\n","\n","LOGGER = logging.getLogger(__name__)\n","IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","IMAGENET_STD = [0.229, 0.224, 0.225]\n","\n","\n","class TBWrapper:\n","    def __init__(self, log_dir):\n","        self.g_iter = 0\n","        self.logger = SummaryWriter(log_dir=log_dir)\n","\n","    def step(self):\n","        self.g_iter += 1\n","\n","\n","class GLASS(torch.nn.Module):\n","    def __init__(self, device):\n","        super(GLASS, self).__init__()\n","        self.device = device\n","        self.input_shape = None\n","        self.pixel_metrics_threshold = 0.5\n","        self.current_epoch_for_vis = 0\n","\n","    def load(\n","            self,\n","            backbone,\n","            layers_to_extract_from,\n","            device,\n","            input_shape,\n","            pretrain_embed_dimension,\n","            target_embed_dimension,\n","            patchsize=3,\n","            patchstride=1,\n","            meta_epochs=640,\n","            eval_epochs=1,\n","            dsc_layers=2,\n","            dsc_hidden=1024,\n","            dsc_margin=0.5,\n","            train_backbone=False,\n","            pre_proj=1,\n","            mining=1,\n","            noise=0.015,\n","            radius=0.75,\n","            p=0.5,\n","            lr=0.0001,\n","            svd=0,\n","            step=20,\n","            limit=392,\n","            **kwargs,\n","    ):\n","\n","        self.backbone = backbone.to(device)\n","        self.layers_to_extract_from = layers_to_extract_from\n","        self.input_shape = input_shape\n","        self.device = device\n","\n","        self.forward_modules = torch.nn.ModuleDict({})\n","        feature_aggregator = common.NetworkFeatureAggregator(\n","            self.backbone, self.layers_to_extract_from, self.device, train_backbone\n","        )\n","        feature_dimensions = feature_aggregator.feature_dimensions(self.input_shape) # (C,H,W) olmalı input_shape\n","        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n","\n","        # Preprocessing modülü, her bir katmandan gelen patch embedding'lerini pretrain_embed_dimension'a (1536) mapler.\n","        self.preprocessing = common.Preprocessing(feature_dimensions, pretrain_embed_dimension)\n","        self.forward_modules[\"preprocessing\"] = self.preprocessing # Atama düzeltildi\n","\n","        self.target_embed_dimension = target_embed_dimension\n","        preadapt_aggregator = common.Aggregator(target_dim=self.target_embed_dimension)\n","        preadapt_aggregator.to(self.device)\n","        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n","\n","        self.meta_epochs = meta_epochs\n","        self.lr = lr\n","        self.train_backbone = train_backbone\n","        if self.train_backbone:\n","            self.backbone_opt = torch.optim.AdamW(self.forward_modules[\"feature_aggregator\"].backbone.parameters(), lr)\n","\n","        self.pre_proj = pre_proj\n","        if self.pre_proj > 0:\n","            self.pre_projection = Projection(self.target_embed_dimension, self.target_embed_dimension, pre_proj)\n","            self.pre_projection.to(self.device)\n","            self.proj_opt = torch.optim.Adam(self.pre_projection.parameters(), lr, weight_decay=1e-5)\n","\n","        self.eval_epochs = eval_epochs\n","        self.dsc_layers = dsc_layers\n","        self.dsc_hidden = dsc_hidden\n","        self.discriminator = Discriminator(self.target_embed_dimension, n_layers=dsc_layers, hidden=dsc_hidden)\n","        self.discriminator.to(self.device)\n","        self.dsc_opt = torch.optim.AdamW(self.discriminator.parameters(), lr=lr * 2)\n","        self.dsc_margin = dsc_margin\n","\n","        self.c = torch.tensor(0.0, device=self.device)\n","        self.p_val = p\n","        self.radius = radius\n","        self.mining = mining\n","        self.noise_val = noise\n","        self.svd = svd\n","        self.step_val = step\n","        self.limit = limit\n","        self.distribution = 0\n","        self.focal_loss = FocalLoss()\n","\n","        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n","\n","        if self.input_shape and len(self.input_shape) == 3:\n","            rescale_target_size = self.input_shape[-2:] # (H, W)\n","        else:\n","            LOGGER.warning(f\"GLASS.load: input_shape {self.input_shape} is not as expected (C,H,W). Using default 256x256 for RescaleSegmentor.\")\n","            rescale_target_size = (256, 256)\n","\n","        self.anomaly_segmentor = common.RescaleSegmentor(device=self.device, target_size=rescale_target_size)\n","\n","        self.model_dir = \"\"\n","        self.dataset_name = \"\"\n","        self.logger = None\n","\n","    def set_model_dir(self, model_dir, dataset_name):\n","        self.model_dir = model_dir\n","        os.makedirs(self.model_dir, exist_ok=True)\n","        self.ckpt_dir = os.path.join(self.model_dir, dataset_name)\n","        os.makedirs(self.ckpt_dir, exist_ok=True)\n","        self.tb_dir = os.path.join(self.ckpt_dir, \"tb\")\n","        os.makedirs(self.tb_dir, exist_ok=True)\n","        self.logger = TBWrapper(self.tb_dir)\n","        LOGGER.info(f\"GLASS Log: Checkpoint directory for '{dataset_name}' set to: {self.ckpt_dir}\")\n","\n","    def _embed(self, images, detach=True, provide_patch_shapes=False, evaluation=False):\n","        if not evaluation and self.train_backbone:\n","            self.forward_modules[\"feature_aggregator\"].train()\n","            features_from_backbone = self.forward_modules[\"feature_aggregator\"](images, eval=False)\n","        else:\n","            self.forward_modules[\"feature_aggregator\"].eval()\n","            with torch.no_grad():\n","                features_from_backbone = self.forward_modules[\"feature_aggregator\"](images, eval=True)\n","\n","        # features_from_backbone, layers_to_extract_from'daki her katman için bir özellik haritası içeren bir sözlüktür.\n","        # Orijinal _embed'deki sırayla (layers_to_extract_from'daki sıra) alalım.\n","        raw_features_list = [features_from_backbone[layer] for layer in self.layers_to_extract_from]\n","\n","        # Her bir özellik haritasını yamala (patchify)\n","        patch_data_list = []\n","        for feat_map in raw_features_list:\n","            # feat_map (B, C_feat, H_feat, W_feat) veya (B, L_transformer, C_transformer) olabilir.\n","            # PatchMaker (B,C,H,W) bekler. Eğer transformer çıktısıysa (B,L,C), (B,C,sqrt(L),sqrt(L)) yap.\n","            current_feat_map = feat_map\n","            if current_feat_map.ndim == 3 and current_feat_map.shape[0] > 0 :\n","                B, L_seq, C_seq = current_feat_map.shape\n","                side_len = int(math.sqrt(L_seq))\n","                if side_len * side_len != L_seq:\n","                    # Bu durumda patchify çalışmaz, ya da bu katman patchify edilmemeli.\n","                    # Şimdilik bu katmanı atlayalım veya hata verelim.\n","                    LOGGER.warning(f\"Skipping feature map with shape {current_feat_map.shape} in _embed as it's 3D and L is not a perfect square.\")\n","                    continue\n","                current_feat_map = current_feat_map.view(B, side_len, side_len, C_seq).permute(0, 3, 1, 2)\n","            elif current_feat_map.ndim != 4:\n","                LOGGER.warning(f\"Skipping feature map with unexpected shape {current_feat_map.shape} in _embed.\")\n","                continue\n","\n","            current_patches, current_shape_info = self.patch_maker.patchify(current_feat_map, return_spatial_info=True)\n","            patch_data_list.append({'patches': current_patches, 'shape_info': current_shape_info})\n","\n","        if not patch_data_list:\n","            LOGGER.error(\"_embed: No valid patch features extracted after patchify stage.\")\n","            return torch.empty(0, device=self.device), [] if provide_patch_shapes else torch.empty(0, device=self.device)\n","\n","        # Referans yama grid boyutları (ilk katmandan)\n","        ref_Hp, ref_Wp = patch_data_list[0]['shape_info']\n","\n","        # Bu liste, her katmandan gelen ve MeanMapper'a gidecek olan yama embedding'lerini tutacak.\n","        # Her bir eleman (B * L_ref, D_raw_patch_embedding) şeklinde olmalı.\n","        embeddings_for_preprocessing = []\n","\n","        for i, data_item in enumerate(patch_data_list):\n","            current_patches_tensor = data_item['patches'] # (B, L_current, C_feat, ps_h, ps_w)\n","            Hp_current, Wp_current = data_item['shape_info']\n","            B, L_current, C_feat, ps_h, ps_w = current_patches_tensor.shape\n","\n","            # Patch embedding'lerini düzleştir: (B, L_current, C_feat*ps_h*ps_w)\n","            flat_patch_embeddings = current_patches_tensor.reshape(B, L_current, -1)\n","\n","            if i == 0: # Referans katman, interpolasyona gerek yok\n","                # (B, L_ref, D_raw) -> (B * L_ref, D_raw)\n","                reshaped_for_mm = flat_patch_embeddings.reshape(-1, flat_patch_embeddings.shape[-1])\n","                embeddings_for_preprocessing.append(reshaped_for_mm)\n","            else:\n","                # Diğer katmanlar için: patch grid'ini referans grid'e interpolate et\n","                # flat_patch_embeddings (B, L_current, D_raw) L_current = Hp_current * Wp_current\n","                # İnterpolasyon için (B, D_raw, Hp_current, Wp_current) şekline ihtiyacımız var\n","                # D_raw = C_feat * ps_h * ps_w\n","                try:\n","                    x_for_interp = flat_patch_embeddings.permute(0, 2, 1).reshape(B, C_feat * ps_h * ps_w, Hp_current, Wp_current)\n","                except RuntimeError as e:\n","                    LOGGER.error(f\"Error reshaping for interpolation in _embed for layer {i}. Shape: {flat_patch_embeddings.shape}, Target: (B, D, {Hp_current}, {Wp_current}). Error: {e}\")\n","                    # Hata durumunda bu katmanı atla veya dummy ekle\n","                    # Şimdilik, bu katman için boş bir tensor ekleyelim, Preprocessing bunu handle etmeli.\n","                    # Veya en iyisi, bu katmandan gelen embedding'leri olduğu gibi (reshape edilmiş) eklemek,\n","                    # Preprocessing'in MeanMapper'ı farklı sayıda patch ile başa çıkamasa da.\n","                    # Bu, stack hatasına geri döner ama en azından burası çökmez.\n","                    # En güvenlisi, bu katmanı atlamak veya dummy eklemek.\n","                    # Şimdilik referans boyutta dummy ekleyelim (bu yanlış sonuç verebilir ama çökmez)\n","                    dummy_interpolated = torch.zeros((B, C_feat * ps_h * ps_w, ref_Hp, ref_Wp), device=self.device)\n","                    reshaped_for_mm = dummy_interpolated.permute(0, 2, 3, 1).reshape(B * ref_Hp * ref_Wp, -1)\n","                    embeddings_for_preprocessing.append(reshaped_for_mm)\n","                    continue\n","\n","\n","                x_interpolated = F.interpolate(\n","                    x_for_interp,\n","                    size=(ref_Hp, ref_Wp),\n","                    mode=\"bilinear\",\n","                    align_corners=False\n","                )\n","                # x_interpolated shape: (B, D_raw, ref_Hp, ref_Wp)\n","\n","                # (B, ref_Hp, ref_Wp, D_raw) -> (B * ref_Hp * ref_Wp, D_raw)\n","                reshaped_for_mm = x_interpolated.permute(0, 2, 3, 1).reshape(B * ref_Hp * ref_Wp, -1)\n","                embeddings_for_preprocessing.append(reshaped_for_mm)\n","\n","        # Şimdi `embeddings_for_preprocessing` listesindeki tüm tensörler\n","        # (B * ref_Hp * ref_Wp, D_raw_embedding_i) şeklinde olmalı.\n","        # MeanMapper her birini (B * ref_Hp * ref_Wp, 1536) yapacak.\n","        # Böylece torch.stack çalışacak.\n","\n","        output_of_preprocessing = self.forward_modules[\"preprocessing\"](embeddings_for_preprocessing)\n","        # Preprocessing çıktısı: (B * ref_Hp * ref_Wp, NumLayers, 1536)\n","\n","        patch_features_aggregated = self.forward_modules[\"preadapt_aggregator\"](output_of_preprocessing)\n","        # Aggregator çıktısı: (B * ref_Hp * ref_Wp, 1536)\n","\n","        original_patch_shapes_to_return = [item['shape_info'] for item in patch_data_list]\n","\n","        if provide_patch_shapes:\n","            return patch_features_aggregated, original_patch_shapes_to_return\n","        return patch_features_aggregated\n","\n","    # trainer, _train_discriminator, tester, _evaluate, _predict, predict metodları\n","    # bir önceki yanıttaki (\"Elbette, `glass.py` dosyasının...\" ile başlayan)\n","    # halleriyle büyük ölçüde aynı kalabilir.\n","    # Sadece _embed çağrılarının (features, shapes) döndürdüğünü ve buna göre\n","    # sadece features kısmının ([0] ile) alındığını kontrol etmek gerekebilir.\n","    # Örneğin, trainer içindeki self.c hesaplamasında:\n","    # current_embeds, _ = self._embed(img_c, evaluation=True, provide_patch_shapes=True)\n","    # Bu zaten doğru yapılmış.\n","    # _train_discriminator içinde de:\n","    # fake_feats_embed, _ = self._embed(aug_imgs, evaluation=False, provide_patch_shapes=True)\n","    # true_feats_embed, _ = self._embed(real_imgs, evaluation=False, provide_patch_shapes=True)\n","    # Bu da doğru.\n","\n","    # Diğer metodlar (trainer, _train_discriminator, tester, _evaluate, _predict, predict)\n","    # için bir önceki yanıttaki (\"Elbette, `glass.py` dosyasının...\" ile başlayan)\n","    # kodları kullanabilirsiniz, o kodlar zaten _embed'in iki değer döndürmesini\n","    # ve metrikleri, logları vs. içeriyordu.\n","    # Aşağıya bu metodların en son hallerini tekrar ekliyorum.\n","\n","    def trainer(self, training_data, val_data, name):\n","        # ... (Bir önceki yanıttaki trainer metodu buraya gelecek)\n","        # İçindeki _embed çağrıları: current_embeds, _ = self._embed(...) şeklinde olacak\n","        state_dict = {}\n","        glob_pattern_trainer = os.path.join(self.ckpt_dir, 'ckpt_best*')\n","        LOGGER.info(f\"GLASS Log: In trainer for '{name}', checking for checkpoints in directory: {self.ckpt_dir}\")\n","        LOGGER.info(f\"GLASS Log: Glob pattern being used: {glob_pattern_trainer}\")\n","        ckpt_path_found = glob.glob(glob_pattern_trainer)\n","        LOGGER.info(f\"GLASS Log: Found checkpoint paths by glob: {ckpt_path_found}\")\n","        ckpt_path_save = os.path.join(self.ckpt_dir, \"ckpt.pth\")\n","\n","        if len(ckpt_path_found) != 0:\n","            LOGGER.warning(f\"GLASS Log: Checkpoint(s) {ckpt_path_found} found. Training will be skipped for {name}.\")\n","            return [0.0] * 8 + [-1]\n","\n","        self.distribution = getattr(training_data.dataset, 'distribution', 0)\n","        xlsx_path = './datasets/excel/' + name.split('_')[0] + '_distribution.xlsx'\n","        try:\n","            if self.distribution == 1: self.svd = 1\n","            elif self.distribution == 2: self.svd = 0\n","            elif self.distribution == 3: self.svd = 1\n","            elif self.distribution == 4 and os.path.exists(xlsx_path):\n","                df = pd.read_excel(xlsx_path)\n","                self.svd = 1 - df.loc[df['Class'] == name, 'Distribution'].values[0]\n","            elif os.path.exists(xlsx_path) and self.distribution != 1: # distribution 0 veya belirtilmemişse dosyadan oku\n","                df = pd.read_excel(xlsx_path)\n","                self.svd = df.loc[df['Class'] == name, 'Distribution'].values[0]\n","            elif self.distribution != 1:\n","                 LOGGER.warning(f\"Distribution excel file not found: {xlsx_path} for {name}. Using svd={self.svd} (from params or default).\")\n","        except Exception as e:\n","            LOGGER.warning(f\"Error processing distribution excel for {name}: {e}. Using svd={self.svd}.\")\n","            if self.distribution == 1: self.svd = 1\n","\n","        if self.distribution == 1 and not ckpt_path_found:\n","            self.forward_modules.eval()\n","            if hasattr(self, 'pre_projection') and self.pre_proj > 0: self.pre_projection.eval()\n","            with torch.no_grad():\n","                img_means = []\n","                for i_c_dist, data_c_dist in enumerate(training_data):\n","                    if i_c_dist > 5 and self.limit > 20 : break # Hızlı bir ortalama için ilk birkaç batch\n","                    img_c_dist = data_c_dist[\"image\"].to(torch.float).to(self.device)\n","                    img_means.append(torch.mean(img_c_dist, dim=0, keepdim=True))\n","                if img_means:\n","                    self.c_img_mean = torch.mean(torch.cat(img_means, dim=0), dim=0)\n","                    avg_img_np = utils.torch_format_2_numpy_img(self.c_img_mean.detach().cpu().numpy())\n","                    self.svd = utils.distribution_judge(avg_img_np, name)\n","                    # avg_img kaydetme\n","                    avg_img_save_path = os.path.join(self.ckpt_dir, f'avg_img_svd{self.svd}_{name}.png')\n","                    os.makedirs(os.path.dirname(avg_img_save_path), exist_ok=True)\n","                    cv2.imwrite(avg_img_save_path, avg_img_np)\n","                else:\n","                    LOGGER.warning(\"No images processed for SVD distribution judgement.\")\n","                    self.svd = 0 # Varsayılan\n","            return self.svd\n","\n","        LOGGER.info(f\"GLASS Log: No 'ckpt_best*' files found for {name}. Proceeding with training.\")\n","        pbar = tqdm.tqdm(range(self.meta_epochs), unit='epoch', desc=f\"Training {name}\")\n","        pbar_str_val_metrics = \"\"\n","        num_metrics = 8\n","        best_record = None\n","        ckpt_path_best_file = \"\"\n","\n","        def update_state_dict_local():\n","            state_dict[\"discriminator\"] = OrderedDict({k: v.detach().cpu() for k, v in self.discriminator.state_dict().items()})\n","            if hasattr(self, 'pre_projection') and self.pre_proj > 0:\n","                state_dict[\"pre_projection\"] = OrderedDict({k: v.detach().cpu() for k, v in self.pre_projection.state_dict().items()})\n","            if self.train_backbone and hasattr(self, 'backbone_opt'): # Backbone parametreleri\n","                 state_dict[\"backbone\"] = OrderedDict({k:v.detach().cpu() for k,v in self.backbone.state_dict().items()})\n","\n","\n","        for i_epoch in pbar:\n","            self.current_epoch_for_vis = i_epoch\n","            self.forward_modules.eval()\n","            if hasattr(self, 'pre_projection') and self.pre_proj > 0: self.pre_projection.eval()\n","\n","            with torch.no_grad():\n","                all_outputs_for_c = []\n","                current_batch_size = training_data.batch_size if hasattr(training_data, 'batch_size') else 1\n","                num_samples_for_c = 0\n","                for i_c, data_c in enumerate(training_data):\n","                    if self.limit > 0 and num_samples_for_c >= self.limit : break\n","                    img_c = data_c[\"image\"].to(torch.float).to(self.device)\n","                    current_embeds, _ = self._embed(img_c, evaluation=True, provide_patch_shapes=True)\n","                    if current_embeds.numel() == 0: continue # _embed boş döndüyse atla\n","\n","                    if hasattr(self, 'pre_projection') and self.pre_proj > 0:\n","                        current_embeds_proj = self.pre_projection(current_embeds)\n","                        current_embeds = current_embeds_proj[0] if isinstance(current_embeds_proj, tuple) and len(current_embeds_proj)==2 else current_embeds_proj\n","                    all_outputs_for_c.append(current_embeds)\n","                    num_samples_for_c += img_c.shape[0]\n","\n","                if all_outputs_for_c:\n","                    concatenated_outputs = torch.cat(all_outputs_for_c, dim=0)\n","                    if concatenated_outputs.numel() > 0: self.c = torch.mean(concatenated_outputs, dim=0)\n","                    else: self.c = torch.zeros(self.target_embed_dimension, device=self.device)\n","                else: self.c = torch.zeros(self.target_embed_dimension, device=self.device)\n","\n","            pbar_str_train_metrics, _, _ = self._train_discriminator(training_data, i_epoch, pbar, pbar_str_val_metrics)\n","            update_state_dict_local()\n","\n","            if (i_epoch + 1) % self.eval_epochs == 0:\n","                images_val, scores_val, segmentations_val, labels_gt_val, masks_gt_val = self.predict(val_data)\n","                eval_results = self._evaluate(images_val, scores_val, segmentations_val, labels_gt_val, masks_gt_val, name, path='validation')\n","                i_auroc, i_ap, i_f1, p_auroc, p_ap, pixel_f1_m, pixel_iou_m, p_pro = eval_results\n","\n","                log_name_prefix = f\"{name}/val\"\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/i-auroc\", i_auroc, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/i-ap\", i_ap, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/i-f1\", i_f1, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/p-auroc\", p_auroc, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/p-ap\", p_ap, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/p-f1\", pixel_f1_m, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/p-iou\", pixel_iou_m, i_epoch)\n","                self.logger.logger.add_scalar(f\"{log_name_prefix}/p-pro\", p_pro, i_epoch)\n","\n","                current_eval_values = [i_auroc, i_ap, i_f1, p_auroc, p_ap, pixel_f1_m, pixel_iou_m, p_pro, i_epoch]\n","                if best_record is None or (i_auroc + p_auroc > best_record[0] + best_record[3]):\n","                    best_record = current_eval_values\n","                    if ckpt_path_best_file and os.path.exists(ckpt_path_best_file):\n","                        try: os.remove(ckpt_path_best_file)\n","                        except OSError as e: LOGGER.warning(f\"Could not remove old best ckpt {ckpt_path_best_file}: {e}\")\n","                    ckpt_path_best_file = os.path.join(self.ckpt_dir, \"ckpt_best_{}.pth\".format(i_epoch))\n","                    torch.save(state_dict, ckpt_path_best_file)\n","                    LOGGER.info(f\"Saved new best checkpoint: {ckpt_path_best_file} (Epoch {i_epoch})\")\n","\n","                pbar_str_val_metrics = (f\" IAUC:{current_eval_values[0]*100:.1f}(B:{best_record[0]*100:.1f})\"\n","                                     f\" IF1:{current_eval_values[2]*100:.1f}(B:{best_record[2]*100:.1f})\"\n","                                     f\" PAUC:{current_eval_values[3]*100:.1f}(B:{best_record[3]*100:.1f})\"\n","                                     f\" PF1:{current_eval_values[5]*100:.1f}(B:{best_record[5]*100:.1f})\"\n","                                     f\" E:{i_epoch}(BE:{best_record[8]})\")\n","\n","            pbar_current_desc = pbar_str_train_metrics + pbar_str_val_metrics\n","            pbar.set_description_str(pbar_current_desc)\n","            torch.save(state_dict, ckpt_path_save)\n","\n","        pbar.close()\n","        if best_record is None:\n","             LOGGER.warning(f\"No evaluation performed or no best record found for {name}. Returning zeros.\")\n","             return [0.0] * num_metrics + [-1]\n","        return best_record\n","\n","    def _train_discriminator(self, input_data, cur_epoch, pbar, pbar_str_val_metrics):\n","        # ... (Bir önceki yanıttaki _train_discriminator metodu, logları içerecek şekilde) ...\n","        # Özellikle true_feats, gaus_feats ve discriminator_input logları kalmalı.\n","        self.forward_modules.eval()\n","        if hasattr(self, 'pre_projection') and self.pre_proj > 0: self.pre_projection.train()\n","        self.discriminator.train()\n","\n","        all_loss, all_pt, all_pf = [], [], []\n","        mean_rt, mean_rg, mean_rf = 0.0, 0.0, 0.0 # Bunlar orijinal SVD mantığından geliyordu, şimdilik 0 kalsın\n","        sample_num = 0\n","        pbar_str_iter_desc = \"\"\n","\n","        for i_iter, data_item in enumerate(input_data):\n","            if self.limit > 0 and sample_num >= self.limit: break\n","\n","            self.dsc_opt.zero_grad()\n","            if hasattr(self, 'proj_opt') and self.pre_proj > 0: self.proj_opt.zero_grad()\n","            if hasattr(self, 'backbone_opt') and self.train_backbone: self.backbone_opt.zero_grad()\n","\n","            aug_imgs = data_item[\"aug\"].to(torch.float).to(self.device)\n","            real_imgs = data_item[\"image\"].to(torch.float).to(self.device)\n","\n","            fake_feats_embed, _ = self._embed(aug_imgs, evaluation=False, provide_patch_shapes=True)\n","            true_feats_embed, _ = self._embed(real_imgs, evaluation=False, provide_patch_shapes=True)\n","\n","            fake_feats, true_feats = fake_feats_embed, true_feats_embed\n","            if hasattr(self, 'pre_projection') and self.pre_proj > 0:\n","                if fake_feats_embed.numel() > 0:\n","                    fake_feats_proj = self.pre_projection(fake_feats_embed)\n","                    fake_feats = fake_feats_proj[0] if isinstance(fake_feats_proj, tuple) and len(fake_feats_proj)==2 and fake_feats_proj[0] is not None else fake_feats_proj\n","                else: fake_feats = fake_feats_embed # Boşsa olduğu gibi bırak\n","\n","                if true_feats_embed.numel() > 0:\n","                    true_feats_proj = self.pre_projection(true_feats_embed)\n","                    true_feats = true_feats_proj[0] if isinstance(true_feats_proj, tuple) and len(true_feats_proj)==2 and true_feats_proj[0] is not None else true_feats_proj\n","                else: true_feats = true_feats_embed\n","\n","            # LOGGER.info(f\"_train_discriminator: Shape of true_feats before cat: {true_feats.shape if isinstance(true_feats, torch.Tensor) else type(true_feats)}\")\n","\n","            # Eğer true_feats boşsa, bu iterasyonu atla\n","            if true_feats.numel() == 0 :\n","                LOGGER.warning(f\"_train_discriminator: true_feats is empty for iter {i_iter}. Skipping batch.\")\n","                sample_num += real_imgs.shape[0]\n","                continue\n","\n","            current_noise = torch.normal(0, self.noise_val, true_feats.shape).to(self.device)\n","            gaus_feats = true_feats + current_noise\n","\n","            # LOGGER.info(f\"_train_discriminator: Shape of gaus_feats before cat: {gaus_feats.shape if isinstance(gaus_feats, torch.Tensor) else type(gaus_feats)}\")\n","\n","            discriminator_input = torch.cat([true_feats, gaus_feats], dim=0)\n","            # LOGGER.info(f\"_train_discriminator: Shape of input to discriminator (after cat): {discriminator_input.shape}\")\n","\n","            scores_disc = self.discriminator(discriminator_input)\n","            s_true = scores_disc[:len(true_feats)]\n","            s_gaus = scores_disc[len(true_feats):]\n","\n","            loss_true_bce = F.binary_cross_entropy(s_true, torch.zeros_like(s_true)) # Sigmoid Discriminator'da var\n","            loss_gaus_bce = F.binary_cross_entropy(s_gaus, torch.ones_like(s_gaus)) # Sigmoid Discriminator'da var\n","            loss = loss_true_bce + loss_gaus_bce\n","\n","            loss.backward()\n","            if hasattr(self, 'proj_opt') and self.pre_proj > 0: self.proj_opt.step()\n","            if hasattr(self, 'backbone_opt') and self.train_backbone: self.backbone_opt.step()\n","            self.dsc_opt.step()\n","\n","            with torch.no_grad():\n","                p_true_val = (s_true.detach() < self.dsc_margin).float().mean() if s_true.numel() > 0 else torch.tensor(0.0)\n","                p_fake_val = (s_gaus.detach() >= self.dsc_margin).float().mean() if s_gaus.numel() > 0 else torch.tensor(0.0)\n","\n","            dataset_name_for_log = input_data.name if hasattr(input_data, 'name') else (input_data.dataset.name if hasattr(input_data, 'dataset') and hasattr(input_data.dataset, 'name') else \"train_iter\")\n","            self.logger.logger.add_scalar(f\"{dataset_name_for_log}/loss\", loss.item(), self.logger.g_iter)\n","            self.logger.logger.add_scalar(f\"{dataset_name_for_log}/p_true\", p_true_val.item(), self.logger.g_iter)\n","            self.logger.logger.add_scalar(f\"{dataset_name_for_log}/p_fake\", p_fake_val.item(), self.logger.g_iter)\n","            self.logger.step()\n","\n","            all_loss.append(loss.item())\n","            all_pt.append(p_true_val.item())\n","            all_pf.append(p_fake_val.item())\n","            sample_num += real_imgs.shape[0]\n","\n","            mean_loss = np.mean(all_loss) if all_loss else 0; mean_pt = np.mean(all_pt) if all_pt else 0; mean_pf = np.mean(all_pf) if all_pf else 0\n","            pbar_str_iter_desc = f\"epoch:{cur_epoch} loss:{mean_loss:.2e} pt:{mean_pt*100:.1f} pf:{mean_pf*100:.1f} svd:{self.svd} sample:{sample_num}/{self.limit}\"\n","            pbar_current_desc = pbar_str_iter_desc + pbar_str_val_metrics\n","            pbar.set_description_str(pbar_current_desc)\n","\n","        return pbar_str_iter_desc, (np.mean(all_pt) if all_pt else 0), (np.mean(all_pf) if all_pf else 0)\n","\n","    def tester(self, test_data, name):\n","        # ... (Bir önceki yanıttaki tester metodu geçerlidir) ...\n","        ckpt_path_list = glob.glob(os.path.join(self.ckpt_dir, 'ckpt_best*'))\n","        epoch_tested = -1\n","        default_metrics_values = [0.0] * 8\n","\n","        if len(ckpt_path_list) > 0:\n","            best_ckpt_file = ckpt_path_list[0]\n","            LOGGER.info(f\"Loading checkpoint for testing {name}: {best_ckpt_file}\")\n","            try:\n","                state_dict_test = torch.load(best_ckpt_file, map_location=self.device)\n","                if 'discriminator' in state_dict_test:\n","                    self.discriminator.load_state_dict(state_dict_test['discriminator'])\n","                    if \"pre_projection\" in state_dict_test and hasattr(self, 'pre_projection') and self.pre_proj > 0:\n","                        self.pre_projection.load_state_dict(state_dict_test[\"pre_projection\"])\n","                elif 'model_state_dict' in state_dict_test:\n","                    self.load_state_dict(state_dict_test['model_state_dict'], strict=False)\n","                else: self.load_state_dict(state_dict_test, strict=False)\n","\n","                images_test, scores_test, segmentations_test, labels_gt_test, masks_gt_test = self.predict(test_data)\n","                eval_results = self._evaluate(images_test, scores_test, segmentations_test, labels_gt_test, masks_gt_test, name, path='eval')\n","                i_auroc, i_ap, i_f1, p_auroc, p_ap, p_f1_m, p_iou_m, p_pro = eval_results\n","                epoch_tested = int(os.path.basename(best_ckpt_file).split('_')[-1].split('.')[0])\n","                return i_auroc, i_ap, i_f1, p_auroc, p_ap, p_f1_m, p_iou_m, p_pro, epoch_tested\n","            except Exception as e: LOGGER.error(f\"Error loading checkpoint or testing for {name}: {e}\"); return default_metrics_values + [epoch_tested]\n","        else: LOGGER.info(f\"No 'ckpt_best*' file found for {name} in {self.ckpt_dir} for testing!\"); return default_metrics_values + [epoch_tested]\n","\n","    def _evaluate(self, images, scores, segmentations, labels_gt, masks_gt, name, path='training'):\n","        # ... (Bir önceki yanıttaki _evaluate metodu geçerlidir) ...\n","        # DEBUG logları isteğe bağlı olarak kaldırılabilir veya tutulabilir.\n","        scores_np = np.squeeze(np.array(scores, dtype=np.float32))\n","        labels_gt_np = np.array(labels_gt, dtype=np.int32)\n","        image_auroc, image_ap, image_f1 = 0.0, 0.0, 0.0\n","        if len(scores_np) > 0 and len(labels_gt_np) > 0 and len(np.unique(labels_gt_np)) > 1 :\n","            try:\n","                image_metrics_dict = metrics.compute_imagewise_retrieval_metrics(scores_np, labels_gt_np, path=path)\n","                image_auroc = image_metrics_dict.get(\"auroc\", 0.0); image_ap = image_metrics_dict.get(\"ap\", 0.0); image_f1 = image_metrics_dict.get(\"max_f1\", 0.0)\n","            except Exception as e: LOGGER.error(f\"Error computing image-wise metrics for {name} ({path}): {e}\")\n","        elif len(scores_np) > 0 and len(labels_gt_np) > 0: LOGGER.warning(f\"Only one class present in labels_gt for {name} ({path}). Image metrics might be 0 or misleading.\")\n","\n","        pixel_auroc, pixel_ap, pixel_f1_m, pixel_iou_m, pixel_pro = -1.0, -1.0, -1.0, -1.0, -1.0\n","        if masks_gt is not None and len(masks_gt) > 0:\n","            segmentations_np = np.array(segmentations, dtype=np.float32)\n","            masks_gt_np = (np.array(masks_gt, dtype=np.float32) > 0.5).astype(np.uint8)\n","            # LOGGER.info(f\"DEBUG in _evaluate: segmentations_np shape: {segmentations_np.shape}, total elements: {segmentations_np.size}\")\n","            # LOGGER.info(f\"DEBUG in _evaluate: masks_gt_np shape: {masks_gt_np.shape}, total elements: {masks_gt_np.size}\")\n","            masks_gt_to_metrics = masks_gt_np.squeeze(1) if masks_gt_np.ndim == 4 and masks_gt_np.shape[1] == 1 else masks_gt_np\n","            segmentations_to_metrics = segmentations_np.squeeze(1) if segmentations_np.ndim == 4 and segmentations_np.shape[1] == 1 else segmentations_np\n","            if segmentations_to_metrics.size>0 and masks_gt_to_metrics.size>0 and segmentations_to_metrics.shape==masks_gt_to_metrics.shape and segmentations_to_metrics.ndim==3 :\n","                try:\n","                    pixel_metrics_dict = metrics.compute_pixelwise_retrieval_metrics(segmentations_to_metrics, masks_gt_to_metrics, path=path, segmentation_threshold=self.pixel_metrics_threshold)\n","                    pixel_auroc = pixel_metrics_dict.get(\"auroc\",0.0); pixel_ap = pixel_metrics_dict.get(\"ap\",0.0); pixel_f1_m = pixel_metrics_dict.get(\"f1\",0.0); pixel_iou_m = pixel_metrics_dict.get(\"iou\",0.0)\n","                except Exception as e: LOGGER.error(f\"Error computing pixel-wise retrieval metrics for {name} ({path}): {e}\")\n","                if path == 'eval':\n","                    try: pixel_pro = metrics.compute_pro(masks_gt_to_metrics, segmentations_to_metrics)\n","                    except Exception as e: LOGGER.error(f\"Error calculating pixel PRO for {name} ({path}): {e}\"); pixel_pro = 0.0\n","            else: LOGGER.warning(f\"Pixel metrics skipped for {name} ({path}) due to empty or mismatched final shapes. Seg shape: {segmentations_to_metrics.shape}, GT shape: {masks_gt_to_metrics.shape}\")\n","\n","        if path != 'training' and len(images) > 0 and ( (len(segmentations) > 0 and len(masks_gt) > 0) or (len(segmentations) == 0 and len(masks_gt) == 0) ) : # Görselleştirme için en azından resimler olmalı\n","            vis_path_root = os.path.join(self.ckpt_dir, 'visualizations', f\"{path}_epoch{self.current_epoch_for_vis}\")\n","            os.makedirs(vis_path_root, exist_ok=True)\n","            num_to_save = min(len(images), 10)\n","            for i_vis in range(num_to_save):\n","                try:\n","                    img_disp = utils.torch_format_2_numpy_img(images[i_vis])\n","                    gt_disp = cv2.cvtColor((masks_gt_np[i_vis].squeeze()*255).astype(np.uint8), cv2.COLOR_GRAY2BGR) if masks_gt and i_vis < len(masks_gt_np) else np.zeros_like(img_disp)\n","                    seg_disp = cv2.applyColorMap((segmentations_np[i_vis].squeeze()*255).astype(np.uint8), cv2.COLORMAP_JET) if segmentations and i_vis < len(segmentations_np) else np.zeros_like(img_disp)\n","                    combined_vis = np.hstack([img_disp, gt_disp, seg_disp])\n","                    cv2.imwrite(os.path.join(vis_path_root, f\"{name}_{str(i_vis + 1).zfill(3)}.png\"), combined_vis)\n","                except Exception as e: LOGGER.error(f\"Error saving visualization for {name} img {i_vis} in {path}: {e}\")\n","        return image_auroc, image_ap, image_f1, pixel_auroc, pixel_ap, pixel_f1_m, pixel_iou_m, pixel_pro\n","\n","    def _predict(self, img_batch_torch):\n","        # ... (Bir önceki yanıttaki _predict metodu geçerlidir) ...\n","        img_batch_torch = img_batch_torch.to(torch.float).to(self.device)\n","        self.forward_modules.eval()\n","        if hasattr(self, 'pre_projection') and self.pre_proj > 0: self.pre_projection.eval()\n","        self.discriminator.eval()\n","\n","        with torch.no_grad():\n","            patch_features, patch_shapes = self._embed(img_batch_torch, provide_patch_shapes=True, evaluation=True)\n","            if patch_features.numel() == 0: # _embed boş döndüyse\n","                LOGGER.warning(\"_predict: _embed returned empty features. Returning zeros.\")\n","                dummy_scores = np.zeros(img_batch_torch.shape[0])\n","                h_dummy, w_dummy = self.input_shape[-2:] if self.input_shape and len(self.input_shape)==3 else (256,256)\n","                dummy_segmentations = [np.zeros((h_dummy,w_dummy)) for _ in range(img_batch_torch.shape[0])]\n","                return list(dummy_scores), dummy_segmentations\n","\n","            patch_features_final = patch_features\n","            if hasattr(self, 'pre_projection') and self.pre_proj > 0:\n","                patch_features_proj = self.pre_projection(patch_features)\n","                patch_features_final = patch_features_proj[0] if isinstance(patch_features_proj, tuple) and len(patch_features_proj) == 2 and patch_features_proj[0] is not None else patch_features_proj\n","\n","            patch_scores_from_discriminator = self.discriminator(patch_features_final)\n","            unpatched_for_segmentation = self.patch_maker.unpatch_scores(patch_scores_from_discriminator.clone(), batchsize=img_batch_torch.shape[0])\n","\n","            patch_scores_heatmap_input = unpatched_for_segmentation\n","            if patch_shapes and patch_shapes[0] and isinstance(patch_shapes[0], (tuple, list)) and len(patch_shapes[0]) == 2:\n","                h_patch, w_patch = patch_shapes[0]\n","                try:\n","                    expected_elements = img_batch_torch.shape[0] * h_patch * w_patch\n","                    if unpatched_for_segmentation.numel() == expected_elements and unpatched_for_segmentation.ndim == 2 and unpatched_for_segmentation.shape[0] == img_batch_torch.shape[0] : # (B, H_patch*W_patch) gibi\n","                        patch_scores_heatmap_input = unpatched_for_segmentation.reshape(img_batch_torch.shape[0], h_patch, w_patch)\n","                    elif unpatched_for_segmentation.ndim == 3 and unpatched_for_segmentation.shape[0] == img_batch_torch.shape[0] and unpatched_for_segmentation.shape[1] == h_patch and unpatched_for_segmentation.shape[2] == w_patch:\n","                        patch_scores_heatmap_input = unpatched_for_segmentation\n","                    else: LOGGER.warning(f\"_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape {unpatched_for_segmentation.shape}) as is.\")\n","                except Exception as e: LOGGER.error(f\"_predict: Error during reshape: {e}. Using unreshaped.\")\n","\n","            # LOGGER.info(f\"DEBUG _predict: Input shape to convert_to_segmentation (patch_scores_heatmap_input): {patch_scores_heatmap_input.shape}\")\n","            segmentation_heatmaps_output = self.anomaly_segmentor.convert_to_segmentation(patch_scores_heatmap_input)\n","\n","            unpatched_for_scoring = self.patch_maker.unpatch_scores(patch_scores_from_discriminator.clone(), batchsize=img_batch_torch.shape[0])\n","            image_level_scores_np = self.patch_maker.score(unpatched_for_scoring)\n","            if isinstance(image_level_scores_np, torch.Tensor): image_level_scores_np = image_level_scores_np.cpu().numpy()\n","\n","        final_segmentation_list = []\n","        if isinstance(segmentation_heatmaps_output, np.ndarray) and segmentation_heatmaps_output.ndim >= 2 :\n","            if segmentation_heatmaps_output.ndim == 3: [final_segmentation_list.append(segmentation_heatmaps_output[i]) for i in range(segmentation_heatmaps_output.shape[0])]\n","            elif segmentation_heatmaps_output.ndim == 4 and segmentation_heatmaps_output.shape[1] == 1: [final_segmentation_list.append(segmentation_heatmaps_output[i].squeeze(0)) for i in range(segmentation_heatmaps_output.shape[0])]\n","            elif segmentation_heatmaps_output.ndim == 2 and img_batch_torch.shape[0] == 1 : final_segmentation_list.append(segmentation_heatmaps_output)\n","            else:\n","                h_def,w_def=(256,256); LOGGER.error(f\"Unexpected segm. shape {segmentation_heatmaps_output.shape}\"); [final_segmentation_list.append(np.zeros((h_def,w_def))) for _ in range(img_batch_torch.shape[0])]\n","        elif isinstance(segmentation_heatmaps_output, list): final_segmentation_list = segmentation_heatmaps_output\n","        else:\n","            h_def,w_def=(256,256); LOGGER.error(f\"Unexpected segm. type {type(segmentation_heatmaps_output)}\"); [final_segmentation_list.append(np.zeros((h_def,w_def))) for _ in range(img_batch_torch.shape[0])]\n","        return list(image_level_scores_np), final_segmentation_list\n","\n","    def predict(self, test_dataloader):\n","        # ... (Bir önceki yanıttaki predict metodu geçerlidir) ...\n","        self.forward_modules.eval()\n","        if hasattr(self, 'pre_projection') and self.pre_proj > 0: self.pre_projection.eval()\n","        self.discriminator.eval()\n","        img_paths_all, images_all, scores_all, masks_pred_all, labels_gt_all, masks_gt_all = [], [], [], [], [], []\n","        with tqdm.tqdm(test_dataloader, desc=\"Inferring...\", leave=False, unit='batch') as data_iterator:\n","            for data in data_iterator:\n","                if isinstance(data, dict):\n","                    labels_gt_all.extend(data[\"is_anomaly\"].numpy().tolist())\n","                    if data.get(\"mask\", None) is not None : masks_gt_all.extend(data[\"mask\"].cpu().numpy().tolist())\n","                    elif data.get(\"mask_gt\", None) is not None : masks_gt_all.extend(data[\"mask_gt\"].cpu().numpy().tolist())\n","                    image_batch_tensor = data[\"image\"]\n","                    image_batch_np = image_batch_tensor.cpu().numpy()\n","                    for j in range(image_batch_np.shape[0]): images_all.append(image_batch_np[j])\n","                    if \"image_path\" in data: img_paths_all.extend(data[\"image_path\"]) # image_path varsa ekle\n","                else: # data doğrudan tensor ise (beklenmedik ama...)\n","                    image_batch_tensor = data.to(torch.float).to(self.device) # device'a gönder\n","                    image_batch_np = image_batch_tensor.cpu().numpy()\n","                    for j in range(image_batch_np.shape[0]): images_all.append(image_batch_np[j])\n","\n","\n","                with torch.no_grad(): _scores_batch, _masks_batch = self._predict(image_batch_tensor)\n","                scores_all.extend(_scores_batch); masks_pred_all.extend(_masks_batch)\n","        return images_all, scores_all, masks_pred_all, labels_gt_all, masks_gt_all"]},{"cell_type":"code","execution_count":49,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1746838960810,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"Fy_OTeY5lA1G","outputId":"7e95652e-6d09-4e6e-aab3-8c23df685ba5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/main.py\n"]}],"source":["#@title main.py\n","# %load main.py\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/main.py\n","\n","from datetime import datetime\n","import pandas as pd\n","import os\n","import logging\n","import sys\n","import click\n","import torch\n","import warnings\n","import backbones\n","import glass # glass.py dosyamızı import ediyoruz\n","import utils\n","import shutil # <--- shutil'i import edin\n","\n","# LOGGER tanımlamasını global kapsamda yapalım\n","LOGGER = logging.getLogger(__name__)\n","\n","@click.group(chain=True)\n","@click.option(\"--results_path\", type=str, default=\"results\", show_default=True)\n","@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n","@click.option(\"--seed\", type=int, default=0, show_default=True)\n","@click.option(\"--log_group\", type=str, default=\"group\", show_default=True)\n","@click.option(\"--log_project\", type=str, default=\"project\", show_default=True)\n","@click.option(\"--run_name\", type=str, default=\"test\", show_default=True)\n","@click.option(\"--test\", type=str, default=\"ckpt\", show_default=True)\n","def main(**kwargs):\n","    \"\"\"Ana CLI grubu.\"\"\"\n","    pass\n","\n","\n","@main.command(\"net\")\n","@click.option(\"--dsc_margin\", type=float, default=0.5, show_default=True)\n","@click.option(\"--train_backbone\", is_flag=True, show_default=True)\n","@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n","@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n","@click.option(\"--pretrain_embed_dimension\", type=int, default=1024, show_default=True)\n","@click.option(\"--target_embed_dimension\", type=int, default=1024, show_default=True)\n","@click.option(\"--patchsize\", type=int, default=3, show_default=True)\n","@click.option(\"--meta_epochs\", type=int, default=640, show_default=True)\n","@click.option(\"--eval_epochs\", type=int, default=1, show_default=True)\n","@click.option(\"--dsc_layers\", type=int, default=2, show_default=True)\n","@click.option(\"--dsc_hidden\", type=int, default=1024, show_default=True)\n","@click.option(\"--pre_proj\", type=int, default=1, show_default=True)\n","@click.option(\"--mining\", type=int, default=1, show_default=True)\n","@click.option(\"--noise\", type=float, default=0.015, show_default=True)\n","@click.option(\"--radius\", type=float, default=0.75, show_default=True)\n","@click.option(\"--p\", type=float, default=0.5, show_default=True)\n","@click.option(\"--lr\", type=float, default=0.0001, show_default=True)\n","@click.option(\"--svd\", type=int, default=0, show_default=True)\n","@click.option(\"--step\", type=int, default=20, show_default=True)\n","@click.option(\"--limit\", type=int, default=392, show_default=True)\n","def net(\n","        backbone_names,\n","        layers_to_extract_from,\n","        pretrain_embed_dimension,\n","        target_embed_dimension,\n","        patchsize,\n","        meta_epochs,\n","        eval_epochs,\n","        dsc_layers,\n","        dsc_hidden,\n","        dsc_margin,\n","        train_backbone,\n","        pre_proj,\n","        mining,\n","        noise,\n","        radius,\n","        p,\n","        lr,\n","        svd,\n","        step,\n","        limit,\n","):\n","    \"\"\"Network yapılandırma komutu.\"\"\"\n","    backbone_names = list(backbone_names)\n","    if len(backbone_names) > 1:\n","        layers_to_extract_from_coll = []\n","        for idx in range(len(backbone_names)):\n","            layers_to_extract_from_coll.append(layers_to_extract_from)\n","    else:\n","        layers_to_extract_from_coll = [layers_to_extract_from]\n","\n","    def get_glass(input_shape_param, device_param): # input_shape, device -> input_shape_param, device_param\n","        glasses = []\n","        for backbone_name, layers_to_extract_from_single in zip(backbone_names, layers_to_extract_from_coll):\n","            backbone_seed = None\n","            if \".seed-\" in backbone_name:\n","                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(backbone_name.split(\"-\")[-1])\n","\n","            current_backbone = backbones.load(backbone_name)\n","            current_backbone.name, current_backbone.seed = backbone_name, backbone_seed\n","\n","            glass_inst = glass.GLASS(device_param) # device_param\n","            glass_inst.load(\n","                backbone=current_backbone,\n","                layers_to_extract_from=layers_to_extract_from_single,\n","                device=device_param, # device_param\n","                input_shape=input_shape_param, # input_shape_param\n","                pretrain_embed_dimension=pretrain_embed_dimension,\n","                target_embed_dimension=target_embed_dimension,\n","                patchsize=patchsize,\n","                meta_epochs=meta_epochs,\n","                eval_epochs=eval_epochs,\n","                dsc_layers=dsc_layers,\n","                dsc_hidden=dsc_hidden,\n","                dsc_margin=dsc_margin,\n","                train_backbone=train_backbone,\n","                pre_proj=pre_proj,\n","                mining=mining,\n","                noise=noise,\n","                radius=radius,\n","                p=p,\n","                lr=lr,\n","                svd=svd,\n","                step=step,\n","                limit=limit,\n","            )\n","            glasses.append(glass_inst.to(device_param)) # device_param\n","        return glasses\n","\n","    return \"get_glass\", get_glass\n","\n","\n","@main.command(\"dataset\")\n","@click.argument(\"name\", type=str)\n","@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n","@click.argument(\"aug_path\", type=click.Path(exists=True, file_okay=False))\n","@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n","@click.option(\"--batch_size\", default=8, type=int, show_default=True)\n","@click.option(\"--num_workers\", default=16, type=int, show_default=True)\n","@click.option(\"--resize\", default=288, type=int, show_default=True)\n","@click.option(\"--imagesize\", default=288, type=int, show_default=True) # Bu imagesize dataset'e gider\n","@click.option(\"--rotate_degrees\", default=0, type=int, show_default=True)\n","@click.option(\"--translate\", default=0, type=float, show_default=True)\n","@click.option(\"--scale\", default=0.0, type=float, show_default=True)\n","@click.option(\"--brightness\", default=0.0, type=float, show_default=True)\n","@click.option(\"--contrast\", default=0.0, type=float, show_default=True)\n","@click.option(\"--saturation\", default=0.0, type=float, show_default=True)\n","@click.option(\"--gray\", default=0.0, type=float, show_default=True)\n","@click.option(\"--hflip\", default=0.0, type=float, show_default=True)\n","@click.option(\"--vflip\", default=0.0, type=float, show_default=True)\n","@click.option(\"--distribution\", default=0, type=int, show_default=True)\n","@click.option(\"--mean\", default=0.5, type=float, show_default=True)\n","@click.option(\"--std\", default=0.1, type=float, show_default=True)\n","@click.option(\"--fg\", default=1, type=int, show_default=True)\n","@click.option(\"--rand_aug\", default=1, type=int, show_default=True)\n","@click.option(\"--downsampling\", default=8, type=int, show_default=True)\n","@click.option(\"--augment\", is_flag=True, show_default=True)\n","def dataset(\n","        name,\n","        data_path,\n","        aug_path,\n","        subdatasets,\n","        batch_size,\n","        resize,\n","        imagesize, # Bu parametre dataset init'e gider\n","        num_workers,\n","        rotate_degrees,\n","        translate,\n","        scale,\n","        brightness,\n","        contrast,\n","        saturation,\n","        gray,\n","        hflip,\n","        vflip,\n","        distribution,\n","        mean,\n","        std,\n","        fg,\n","        rand_aug,\n","        downsampling,\n","        augment,\n","):\n","    \"\"\"Dataset yapılandırma komutu.\"\"\"\n","    _DATASETS = {\"mvtec\": [\"datasets.mvtec\", \"MVTecDataset\"], \"visa\": [\"datasets.visa\", \"VisADataset\"],\n","                 \"mpdd\": [\"datasets.mvtec\", \"MVTecDataset\"], \"wfdd\": [\"datasets.mvtec\", \"MVTecDataset\"], }\n","    dataset_info = _DATASETS[name]\n","    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n","\n","    def get_dataloaders(seed_val, test_mode, get_name_dl=name): # get_name -> get_name_dl\n","        dataloaders_list = []\n","        for subdataset_name in subdatasets:\n","            # Dataset sınıfına imagesize parametresini doğru şekilde veriyoruz.\n","            # Dataset sınıfının __init__ metodunun 'imagesize' parametresini nasıl aldığına bağlı.\n","            # Genellikle int (kare için) veya tuple (H,W) alır.\n","            test_dataset = dataset_library.__dict__[dataset_info[1]](\n","                source=data_path, # source=data_path (orijinaldeki gibi)\n","                anomaly_source_path=aug_path, # anomaly_source_path=aug_path\n","                classname=subdataset_name,\n","                resize=resize, # int\n","                imagesize=imagesize, # int (dataset bunu (H,W) tuple'ına çevirebilir)\n","                split=dataset_library.DatasetSplit.TEST,\n","                seed=seed_val,\n","                # Orijinal dataset'te olmayan parametreleri çıkaralım veya None yapalım\n","                # distribution=distribution, mean=mean, std=std, fg=fg, rand_aug=rand_aug,\n","                # downsampling=downsampling, augment=augment, batch_size=batch_size gibi parametreler\n","                # dataset __init__'inde tanımlı değilse hata verir.\n","                # Bunlar genellikle transformlar içinde veya trainer'da kullanılır.\n","                # Şimdilik dataset __init__'inin sadece gerekli olanları aldığını varsayalım.\n","            )\n","\n","            test_dataloader = torch.utils.data.DataLoader(\n","                test_dataset,\n","                batch_size=batch_size,\n","                shuffle=False,\n","                num_workers=num_workers,\n","                prefetch_factor=2,\n","                pin_memory=True,\n","            )\n","            test_dataloader.name = get_name_dl + \"_\" + subdataset_name\n","\n","            if test_mode == 'ckpt':\n","                train_dataset_params = {\n","                    'source': data_path,\n","                    'anomaly_source_path': aug_path,\n","                    'classname': subdataset_name,\n","                    'resize': resize,\n","                    'imagesize': imagesize,\n","                    'split': dataset_library.DatasetSplit.TRAIN,\n","                    'seed': seed_val,\n","                    'rotate_degrees': rotate_degrees,\n","                    'translate': translate,\n","                    'brightness_factor': brightness,\n","                    'contrast_factor': contrast,\n","                    'saturation_factor': saturation,\n","                    'gray_p': gray,\n","                    'h_flip_p': hflip,\n","                    'v_flip_p': vflip,\n","                    'scale': scale,\n","                    'augment': augment, # Bu bir flag, doğrudan dataset'e gidebilir\n","                    # Aşağıdakiler genellikle dataset'e gitmez, transformlarda veya trainer'da kullanılır:\n","                    # 'distribution': distribution,\n","                    # 'mean': mean, 'std': std,\n","                    # 'fg': fg, 'rand_aug': rand_aug,\n","                    # 'downsampling': downsampling,\n","                    # 'batch_size': batch_size # Bu DataLoader'a aittir.\n","                }\n","                # Dataset sınıfınızın __init__ metoduna göre bu parametreleri filtreleyin.\n","                # Örneğin, eğer dataset'inizde 'dataset_name' gibi bir argüman yoksa kaldırın.\n","                # 'distribution', 'fg', 'rand_aug', 'downsampling' gibi parametreler dataset'inize\n","                # doğrudan gitmiyorsa, bunları da kaldırın.\n","                # Örnek: MVTecDataset sadece source, classname, resize, imagesize, split, seed, transform vs. alabilir.\n","                # Bu kısmı kendi dataset sınıfınızın aldığı argümanlara göre düzenlemeniz en sağlıklısı.\n","                # Şimdilik, dataset'in bu parametreleri kabul ettiğini varsayıyoruz.\n","                # Eğer kabul etmiyorsa, dataset sınıfınızın __init__ metodunu kontrol edin.\n","\n","                train_dataset = dataset_library.__dict__[dataset_info[1]](**train_dataset_params)\n","\n","\n","                train_dataloader = torch.utils.data.DataLoader(\n","                    train_dataset,\n","                    batch_size=batch_size,\n","                    shuffle=True,\n","                    num_workers=num_workers,\n","                    prefetch_factor=2,\n","                    pin_memory=True,\n","                )\n","                train_dataloader.name = test_dataloader.name\n","                LOGGER.info(f\"Dataset {subdataset_name.upper():^20}: train={len(train_dataset)} test={len(test_dataset)}\")\n","            else:\n","                train_dataloader = test_dataloader\n","                LOGGER.info(f\"Dataset {subdataset_name.upper():^20}: train=0 (using test data for placeholder or no training) test={len(test_dataset)}\")\n","\n","            dataloader_dict = {\n","                \"training\": train_dataloader,\n","                \"testing\": test_dataloader,\n","            }\n","            dataloaders_list.append(dataloader_dict)\n","\n","        print(\"\\n\")\n","        return dataloaders_list\n","\n","    return \"get_dataloaders\", get_dataloaders\n","\n","\n","@main.result_callback()\n","def run(\n","        methods,\n","        results_path,\n","        gpu,\n","        seed,\n","        log_project,\n","        log_group,\n","        run_name,\n","        test,\n","):\n","    \"\"\"Ana çalıştırma ve sonuç toplama fonksiyonu.\"\"\"\n","    methods_dict = {key: item for (key, item) in methods}\n","\n","    unique_run_save_path = os.path.join(results_path, log_project, log_group, run_name)\n","\n","    if test == 'ckpt' and os.path.exists(unique_run_save_path):\n","        LOGGER.info(f\"Overwrite mode: Deleting existing run directory for training: {unique_run_save_path}\")\n","        shutil.rmtree(unique_run_save_path, ignore_errors=True)\n","\n","    os.makedirs(unique_run_save_path, exist_ok=True)\n","    LOGGER.info(f\"Run results will be saved in: {unique_run_save_path}\")\n","\n","    list_of_dataloaders = methods_dict[\"get_dataloaders\"](seed_val=seed, test_mode=test)\n","    device = utils.set_torch_device(gpu)\n","\n","    result_collect = []\n","    all_distribution_data = []\n","\n","    for dataloader_count, dataloaders_for_class in enumerate(list_of_dataloaders):\n","        utils.fix_seeds(seed, device)\n","        dataset_name_from_loader = dataloaders_for_class[\"training\"].name\n","\n","        img_size_ds_raw = dataloaders_for_class[\"training\"].dataset.imagesize\n","        LOGGER.info(f\"DEBUG main.py run: Raw imagesize from dataset (dataset.imagesize): {img_size_ds_raw}, type: {type(img_size_ds_raw)}\")\n","\n","        img_size_for_shape_calc = img_size_ds_raw\n","        if isinstance(img_size_ds_raw, (list, tuple)) and len(img_size_ds_raw) == 2 and img_size_ds_raw[0] < 10 : # Genellikle kanal sayısı 3 veya 1 olur\n","            LOGGER.warning(f\"DEBUG main.py run: Assuming dataset.imagesize {img_size_ds_raw} is (C, Dim). Using Dim ({img_size_ds_raw[1]}) for square size.\")\n","            img_size_for_shape_calc = img_size_ds_raw[1]\n","\n","        if isinstance(img_size_for_shape_calc, (list, tuple)) and len(img_size_for_shape_calc) == 2:\n","            current_input_shape = (3, img_size_for_shape_calc[0], img_size_for_shape_calc[1])\n","        elif isinstance(img_size_for_shape_calc, int):\n","            current_input_shape = (3, img_size_for_shape_calc, img_size_for_shape_calc)\n","        else:\n","            default_img_size = 256 # Güvenli bir varsayılan\n","            LOGGER.error(f\"DEBUG main.py run: Could not determine input_shape from imagesize: {img_size_for_shape_calc}. Using default {default_img_size}x{default_img_size}.\")\n","            current_input_shape = (3, default_img_size, default_img_size)\n","\n","        LOGGER.info(f\"DEBUG main.py run: current_input_shape for GLASS.load: {current_input_shape}\")\n","\n","        glass_list = methods_dict[\"get_glass\"](input_shape_param=current_input_shape, device_param=device) # input_shape, device -> _param\n","\n","        LOGGER.info(\n","            \"Selecting dataset [{}] ({}/{}) {}\".format(\n","                dataset_name_from_loader,\n","                dataloader_count + 1,\n","                len(list_of_dataloaders),\n","                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            )\n","        )\n","\n","        models_dir_in_main = os.path.join(unique_run_save_path, \"models\")\n","\n","        for i, current_glass_model in enumerate(glass_list):\n","            if current_glass_model.backbone.seed is not None:\n","                utils.fix_seeds(current_glass_model.backbone.seed, device)\n","\n","            specific_model_path = os.path.join(models_dir_in_main, f\"backbone_{i}\")\n","            current_glass_model.set_model_dir(specific_model_path, dataset_name_from_loader)\n","\n","            if test == 'ckpt':\n","                training_outcome = current_glass_model.trainer(\n","                    dataloaders_for_class[\"training\"],\n","                    dataloaders_for_class[\"testing\"],\n","                    dataset_name_from_loader\n","                )\n","\n","                if isinstance(training_outcome, int):\n","                    svd_value_from_trainer = training_outcome\n","                    all_distribution_data.append({\n","                        'Class': dataset_name_from_loader,\n","                        'Distribution': svd_value_from_trainer,\n","                        'Foreground': svd_value_from_trainer\n","                    })\n","                    LOGGER.info(f\"Distribution judgement for {dataset_name_from_loader}: SVD={svd_value_from_trainer}. Skipping testing for this class.\")\n","                    continue\n","                else:\n","                    if training_outcome and len(training_outcome) == 9:\n","                        i_auroc, i_ap, i_f1_val, p_auroc, p_ap, p_f1_val, p_iou_val, p_pro, epoch_val = training_outcome\n","\n","                        result_dict_current = {\n","                            \"dataset_name\": dataset_name_from_loader,\n","                            \"image_auroc\": i_auroc, \"image_ap\": i_ap, \"image_f1\": i_f1_val,\n","                            \"pixel_auroc\": p_auroc, \"pixel_ap\": p_ap, \"pixel_f1\": p_f1_val,\n","                            \"pixel_iou\": p_iou_val, \"pixel_pro\": p_pro,\n","                            \"best_epoch\": epoch_val,\n","                        }\n","                        result_collect.append(result_dict_current)\n","                    else:\n","                        LOGGER.error(f\"Unexpected outcome from trainer for {dataset_name_from_loader}: {training_outcome}\")\n","                        continue\n","            else:\n","                 LOGGER.info(f\"Running tester for pre-trained model on {dataset_name_from_loader}.\")\n","                 test_metrics = current_glass_model.tester(\n","                     dataloaders_for_class[\"testing\"], dataset_name_from_loader\n","                 )\n","                 if test_metrics and len(test_metrics) == 9:\n","                    i_auroc, i_ap, i_f1_test, p_auroc, p_ap, p_f1_test, p_iou_test, p_pro, epoch_test = test_metrics\n","                    result_dict_current = {\n","                        \"dataset_name\": dataset_name_from_loader,\n","                        \"image_auroc\": i_auroc, \"image_ap\": i_ap, \"image_f1\": i_f1_test,\n","                        \"pixel_auroc\": p_auroc, \"pixel_ap\": p_ap, \"pixel_f1\": p_f1_test,\n","                        \"pixel_iou\": p_iou_test, \"pixel_pro\": p_pro,\n","                        \"best_epoch\": epoch_test,\n","                    }\n","                    result_collect.append(result_dict_current)\n","                 else:\n","                    LOGGER.error(f\"Unexpected outcome from tester for {dataset_name_from_loader}: {test_metrics}\")\n","                    continue\n","\n","            if result_collect and result_collect[-1][\"dataset_name\"] == dataset_name_from_loader :\n","                LOGGER.info(f\"Results for {dataset_name_from_loader}:\")\n","                current_result_to_print = result_collect[-1]\n","                if current_result_to_print.get(\"best_epoch\", -1) > -1 :\n","                    output_str_parts = []\n","                    for key, item in current_result_to_print.items():\n","                        if isinstance(item, str):\n","                            continue\n","                        elif isinstance(item, int):\n","                            output_str_parts.append(f\"{key}: {item}\")\n","                        else:\n","                            output_str_parts.append(f\"{key}: {item * 100:.2f}\")\n","                    print(\"  \" + \" | \".join(output_str_parts))\n","                    print(\"-\" * 50)\n","\n","                    if result_collect :\n","                        csv_metric_names = list(result_collect[0].keys())[1:]\n","                        csv_dataset_names = [res[\"dataset_name\"] for res in result_collect]\n","                        csv_scores = [list(res.values())[1:] for res in result_collect]\n","\n","                        utils.compute_and_store_final_results(\n","                            unique_run_save_path,\n","                            csv_scores,\n","                            csv_metric_names,\n","                            row_names=csv_dataset_names,\n","                        )\n","\n","    if all_distribution_data:\n","        df_dist = pd.DataFrame(all_distribution_data)\n","        if not df_dist.empty:\n","            base_dataset_name_for_excel = \"unknown_dataset\"\n","            if 'dataset_name_from_loader' in locals() and dataset_name_from_loader:\n","                 base_dataset_name_for_excel = dataset_name_from_loader.split('_')[0] if '_' in dataset_name_from_loader else dataset_name_from_loader\n","\n","            excel_dir = './datasets/excel'\n","            os.makedirs(excel_dir, exist_ok=True)\n","            xlsx_path = os.path.join(excel_dir, f'{base_dataset_name_for_excel}_distribution.xlsx')\n","            try:\n","                df_dist.to_excel(xlsx_path, index=False)\n","                LOGGER.info(f\"Distribution judgment saved to {xlsx_path}\")\n","            except Exception as e:\n","                LOGGER.error(f\"Could not save distribution judgment to {xlsx_path}: {e}\")\n","\n","\n","if __name__ == \"__main__\":\n","    warnings.filterwarnings('ignore')\n","    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')\n","    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n","    main()"]},{"cell_type":"code","execution_count":56,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1746841823143,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"MimUjPFHz07N","outputId":"cdbdb81d-a4fa-4e06-fe17-cf3592a0c7f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/GLASS_09_05_2025/GLASS/shell/run-mvtec.sh\n"]}],"source":["#@title run-mvtec.sh\n","# %load shell/run-mvtec.sh\n","%%writefile /content/drive/MyDrive/GLASS_09_05_2025/GLASS/shell/run-mvtec.sh\n","\n","datapath=/content/drive/MyDrive/GLASS_09_05_2025/GLASS/mvtec_anomaly_detection\n","augpath=/content/drive/MyDrive/GLASS_09_05_2025/dtd/images\n","classes=('wood')\n","flags=($(for class in \"${classes[@]}\"; do echo '-d '\"${class}\"; done))\n","\n","cd /content/drive/MyDrive/GLASS_09_05_2025/GLASS\n","python main.py \\\n","    --gpu 0 \\\n","    --seed 0 \\\n","    --test ckpt \\\n","    --run_name wood_exp_3 \\  #her train öncesi sayıyı arttır\n","  net \\\n","    -b wideresnet50 \\\n","    -le layer2 \\\n","    -le layer3 \\\n","    --pretrain_embed_dimension 1536 \\\n","    --target_embed_dimension 1536 \\\n","    --patchsize 3 \\\n","    --meta_epochs 25 \\\n","    --eval_epochs 1 \\\n","    --dsc_layers 2 \\\n","    --dsc_hidden 1024 \\\n","    --pre_proj 1 \\\n","    --mining 1 \\\n","    --noise 0.015 \\\n","    --radius 0.75 \\\n","    --p 0.5 \\\n","    --step 20 \\\n","    --limit 392 \\\n","  dataset \\\n","    --distribution 0 \\\n","    --mean 0.5 \\\n","    --std 0.1 \\\n","    --fg 0 \\\n","    --rand_aug 1 \\\n","    --batch_size 16 \\\n","    --resize 256 \\\n","    --imagesize 256 \"${flags[@]}\" mvtec $datapath $augpath"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2819,"status":"ok","timestamp":1746839347747,"user":{"displayName":"özce yune","userId":"13987651626877442442"},"user_tz":-180},"id":"7h71-sG8iTwq","outputId":"5a86779d-9ab1-4dee-8133-3236824211c3","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GLASS_09_05_2025/GLASS/shell\n"]}],"source":["#@title trainingi compile et\n","# train kodunu compile et\n","%cd /content/drive/MyDrive/GLASS_09_05_2025/GLASS/shell/\n","!conda run -n GLASS chmod +x run-mvtec.sh"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAtKSI6Xij6P","collapsed":true,"cellView":"form","executionInfo":{"status":"ok","timestamp":1746841202535,"user_tz":-180,"elapsed":1853664,"user":{"displayName":"özce yune","userId":"13987651626877442442"}},"outputId":"a3b5ea4e-cf0b-4a3a-c65f-c3f4c19144ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:glass:Saved new best checkpoint: results/project/group/wood_exp_3/models/backbone_0/mvtec_wood/ckpt_best_14.pth (Epoch 14)\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:glass:Saved new best checkpoint: results/project/group/wood_exp_3/models/backbone_0/mvtec_wood/ckpt_best_18.pth (Epoch 18)\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:glass:Saved new best checkpoint: results/project/group/wood_exp_3/models/backbone_0/mvtec_wood/ckpt_best_24.pth (Epoch 24)\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([32768, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([32768, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([32768, 1])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:common:Aggregator input features shape: torch.Size([6144, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([6144, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([6144, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([6144, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([6144, 1536])\n","INFO:model:Discriminator input shape: torch.Size([12288, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([12288, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([12288, 1])\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([16384, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([16384, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([16384, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([16384, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([16384, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([16384, 1536])\n","INFO:model:Discriminator input shape: torch.Size([16384, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([16384, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([16384, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([16, 1024, 1])) as is.\n","INFO:common:Aggregator input features shape: torch.Size([13312, 2, 1536])\n","INFO:common:Aggregator after reshape to (NumPatches, 1, -1): torch.Size([13312, 1, 3072])\n","INFO:common:Aggregator after adaptive_avg_pool1d (target_dim=1536): torch.Size([13312, 1, 1536])\n","INFO:common:Aggregator final output shape: torch.Size([13312, 1536])\n","INFO:model:Projection input shape (actual x.shape): torch.Size([13312, 1536])\n","INFO:model:Projection output shape (actual processed_x.shape): torch.Size([13312, 1536])\n","INFO:model:Discriminator input shape: torch.Size([13312, 1536])\n","INFO:model:Discriminator after body shape: torch.Size([13312, 1024])\n","INFO:model:Discriminator final output shape: torch.Size([13312, 1])\n","WARNING:glass:_predict: Shape mismatch for reshape. Using unpatched_for_segmentation (shape torch.Size([13, 1024, 1])) as is.\n","INFO:__main__:Results for mvtec_wood:\n","  image_auroc: 80.12 | image_ap: 73.32 | image_f1: 78.95 | pixel_auroc: 73.61 | pixel_ap: 4.92 | pixel_f1: 7.65 | pixel_iou: 3.98 | pixel_pro: -100.00 | best_epoch: 24\n","--------------------------------------------------\n","\n","\n","Training mvtec_wood:   0%|          | 0/35 [00:00<?, ?epoch/s]\n","epoch:0 loss:1.60e+00 pt:35.2 pf:64.3 svd:0 sample:16/392:   0%|          | 0/35 [00:18<?, ?epoch/s]\n","epoch:0 loss:1.61e+00 pt:36.7 pf:63.1 svd:0 sample:32/392:   0%|          | 0/35 [00:19<?, ?epoch/s]\n","epoch:0 loss:1.59e+00 pt:38.4 pf:61.6 svd:0 sample:48/392:   0%|          | 0/35 [00:19<?, ?epoch/s]\n","epoch:0 loss:1.57e+00 pt:40.2 pf:60.1 svd:0 sample:64/392:   0%|          | 0/35 [00:20<?, ?epoch/s]\n","epoch:0 loss:1.56e+00 pt:41.7 pf:58.9 svd:0 sample:70/392:   0%|          | 0/35 [00:20<?, ?epoch/s]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:06,  8.30s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:25,  3.58s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.07s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:06,  1.37s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:03,  1.03batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.36batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.69batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.96batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.31batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:0 loss:1.56e+00 pt:41.7 pf:58.9 svd:0 sample:70/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   0%|          | 0/35 [00:42<?, ?epoch/s]\n","epoch:0 loss:1.56e+00 pt:41.7 pf:58.9 svd:0 sample:70/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [00:42<24:12, 42.73s/epoch]\n","epoch:1 loss:1.49e+00 pt:47.2 pf:54.4 svd:0 sample:16/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [01:11<24:12, 42.73s/epoch]\n","epoch:1 loss:1.48e+00 pt:48.6 pf:53.5 svd:0 sample:32/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [01:13<24:12, 42.73s/epoch]\n","epoch:1 loss:1.48e+00 pt:49.0 pf:53.2 svd:0 sample:48/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [01:13<24:12, 42.73s/epoch]\n","epoch:1 loss:1.47e+00 pt:49.9 pf:52.6 svd:0 sample:64/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [01:14<24:12, 42.73s/epoch]\n","epoch:1 loss:1.47e+00 pt:50.1 pf:52.4 svd:0 sample:70/392 IAUC:49.3(B:49.3) IF1:67.0(B:67.0) PAUC:42.2(B:42.2) PF1:0.0(B:0.0) E:0(BE:0):   3%|▎         | 1/35 [01:14<24:12, 42.73s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:17,  9.65s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:09<00:29,  4.15s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.38s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:08,  1.68s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.17s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.15batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.49batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:11<00:00,  1.83batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.19batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:1 loss:1.47e+00 pt:50.1 pf:52.4 svd:0 sample:70/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   3%|▎         | 1/35 [01:39<24:12, 42.73s/epoch]\n","epoch:1 loss:1.47e+00 pt:50.1 pf:52.4 svd:0 sample:70/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [01:39<28:07, 51.14s/epoch]\n","epoch:2 loss:1.46e+00 pt:51.4 pf:51.9 svd:0 sample:16/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [02:07<28:07, 51.14s/epoch]\n","epoch:2 loss:1.45e+00 pt:53.3 pf:50.7 svd:0 sample:32/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [02:08<28:07, 51.14s/epoch]\n","epoch:2 loss:1.44e+00 pt:54.4 pf:50.2 svd:0 sample:48/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [02:10<28:07, 51.14s/epoch]\n","epoch:2 loss:1.44e+00 pt:55.2 pf:49.9 svd:0 sample:64/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [02:10<28:07, 51.14s/epoch]\n","epoch:2 loss:1.43e+00 pt:55.8 pf:49.8 svd:0 sample:70/392 IAUC:53.7(B:53.7) IF1:68.7(B:68.7) PAUC:52.8(B:52.8) PF1:0.0(B:0.0) E:1(BE:1):   6%|▌         | 2/35 [02:11<28:07, 51.14s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:24, 10.62s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:31,  4.57s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.64s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.72s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.23s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.09batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.38batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.71batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:13<00:00,  2.08batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:2 loss:1.43e+00 pt:55.8 pf:49.8 svd:0 sample:70/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   6%|▌         | 2/35 [02:37<28:07, 51.14s/epoch]\n","epoch:2 loss:1.43e+00 pt:55.8 pf:49.8 svd:0 sample:70/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [02:37<28:55, 54.25s/epoch]\n","epoch:3 loss:1.41e+00 pt:57.2 pf:49.9 svd:0 sample:16/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [03:05<28:55, 54.25s/epoch]\n","epoch:3 loss:1.41e+00 pt:56.1 pf:50.8 svd:0 sample:32/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [03:05<28:55, 54.25s/epoch]\n","epoch:3 loss:1.41e+00 pt:54.8 pf:52.2 svd:0 sample:48/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [03:06<28:55, 54.25s/epoch]\n","epoch:3 loss:1.40e+00 pt:54.4 pf:53.1 svd:0 sample:64/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [03:06<28:55, 54.25s/epoch]\n","epoch:3 loss:1.40e+00 pt:54.0 pf:53.7 svd:0 sample:70/392 IAUC:45.2(B:53.7) IF1:67.0(B:68.7) PAUC:45.7(B:52.8) PF1:0.0(B:0.0) E:2(BE:1):   9%|▊         | 3/35 [03:06<28:55, 54.25s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:23, 10.49s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:31,  4.49s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.57s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.67s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.17s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.15batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.48batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.80batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.19batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:3 loss:1.40e+00 pt:54.0 pf:53.7 svd:0 sample:70/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):   9%|▊         | 3/35 [03:32<28:55, 54.25s/epoch]\n","epoch:3 loss:1.40e+00 pt:54.0 pf:53.7 svd:0 sample:70/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [03:32<28:06, 54.39s/epoch]\n","epoch:4 loss:1.38e+00 pt:51.7 pf:57.8 svd:0 sample:16/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [04:01<28:06, 54.39s/epoch]\n","epoch:4 loss:1.39e+00 pt:50.3 pf:58.8 svd:0 sample:32/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [04:02<28:06, 54.39s/epoch]\n","epoch:4 loss:1.39e+00 pt:48.8 pf:60.1 svd:0 sample:48/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [04:02<28:06, 54.39s/epoch]\n","epoch:4 loss:1.38e+00 pt:48.5 pf:60.6 svd:0 sample:64/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [04:03<28:06, 54.39s/epoch]\n","epoch:4 loss:1.38e+00 pt:48.9 pf:61.0 svd:0 sample:70/392 IAUC:48.0(B:53.7) IF1:68.0(B:68.7) PAUC:43.7(B:52.8) PF1:0.0(B:0.0) E:3(BE:1):  11%|█▏        | 4/35 [04:03<28:06, 54.39s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:12<01:37, 12.17s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:12<00:36,  5.21s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:12<00:17,  2.98s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:13<00:09,  1.91s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:13<00:05,  1.34s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:13<00:02,  1.02batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:14<00:01,  1.31batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:14<00:00,  1.63batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:14<00:00,  1.99batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:4 loss:1.38e+00 pt:48.9 pf:61.0 svd:0 sample:70/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  11%|█▏        | 4/35 [04:32<28:06, 54.39s/epoch]\n","epoch:4 loss:1.38e+00 pt:48.9 pf:61.0 svd:0 sample:70/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [04:32<28:16, 56.56s/epoch]\n","epoch:5 loss:1.36e+00 pt:47.5 pf:64.1 svd:0 sample:16/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [05:00<28:16, 56.56s/epoch]\n","epoch:5 loss:1.37e+00 pt:48.0 pf:63.6 svd:0 sample:32/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [05:00<28:16, 56.56s/epoch]\n","epoch:5 loss:1.36e+00 pt:48.0 pf:63.9 svd:0 sample:48/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [05:01<28:16, 56.56s/epoch]\n","epoch:5 loss:1.36e+00 pt:48.2 pf:64.0 svd:0 sample:64/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [05:01<28:16, 56.56s/epoch]\n","epoch:5 loss:1.36e+00 pt:49.0 pf:63.8 svd:0 sample:70/392 IAUC:44.3(B:53.7) IF1:67.0(B:68.7) PAUC:41.9(B:52.8) PF1:0.0(B:0.0) E:4(BE:1):  14%|█▍        | 5/35 [05:01<28:16, 56.56s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:01,  7.75s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:23,  3.38s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:11,  2.00s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:06,  1.34s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:03,  1.02batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.32batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.61batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.90batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.24batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:5 loss:1.36e+00 pt:49.0 pf:63.8 svd:0 sample:70/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  14%|█▍        | 5/35 [05:23<28:16, 56.56s/epoch]\n","epoch:5 loss:1.36e+00 pt:49.0 pf:63.8 svd:0 sample:70/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:23<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:51.5 pf:64.3 svd:0 sample:16/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:51<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:51.7 pf:64.8 svd:0 sample:32/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:52<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:52.0 pf:64.6 svd:0 sample:48/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:52<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:52.7 pf:64.3 svd:0 sample:64/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:53<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:53.6 pf:63.5 svd:0 sample:70/392 IAUC:56.5(B:53.7) IF1:67.0(B:68.7) PAUC:47.2(B:52.8) PF1:0.0(B:0.0) E:5(BE:1):  17%|█▋        | 6/35 [05:53<26:21, 54.52s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:04,  8.10s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:24,  3.52s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.04s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:07,  1.48s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.05s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.25batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.58batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.90batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.29batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:6 loss:1.34e+00 pt:53.6 pf:63.5 svd:0 sample:70/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  17%|█▋        | 6/35 [06:16<26:21, 54.52s/epoch]\n","epoch:6 loss:1.34e+00 pt:53.6 pf:63.5 svd:0 sample:70/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:16<25:17, 54.20s/epoch]\n","epoch:7 loss:1.34e+00 pt:56.4 pf:60.9 svd:0 sample:16/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:47<25:17, 54.20s/epoch]\n","epoch:7 loss:1.34e+00 pt:55.2 pf:61.9 svd:0 sample:32/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:47<25:17, 54.20s/epoch]\n","epoch:7 loss:1.33e+00 pt:55.9 pf:62.0 svd:0 sample:48/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:48<25:17, 54.20s/epoch]\n","epoch:7 loss:1.33e+00 pt:56.3 pf:62.2 svd:0 sample:64/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:48<25:17, 54.20s/epoch]\n","epoch:7 loss:1.33e+00 pt:56.5 pf:62.4 svd:0 sample:70/392 IAUC:45.8(B:53.7) IF1:67.3(B:68.7) PAUC:36.9(B:52.8) PF1:0.0(B:0.0) E:6(BE:1):  20%|██        | 7/35 [06:48<25:17, 54.20s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:25, 10.70s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:32,  4.58s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.62s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.70s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.20s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.13batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.43batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.77batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:13<00:00,  2.14batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:7 loss:1.33e+00 pt:56.5 pf:62.4 svd:0 sample:70/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  20%|██        | 7/35 [07:15<25:17, 54.20s/epoch]\n","epoch:7 loss:1.33e+00 pt:56.5 pf:62.4 svd:0 sample:70/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:15<25:01, 55.59s/epoch]\n","epoch:8 loss:1.31e+00 pt:59.3 pf:64.2 svd:0 sample:16/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:42<25:01, 55.59s/epoch]\n","epoch:8 loss:1.32e+00 pt:58.1 pf:64.1 svd:0 sample:32/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:42<25:01, 55.59s/epoch]\n","epoch:8 loss:1.32e+00 pt:58.4 pf:63.3 svd:0 sample:48/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:43<25:01, 55.59s/epoch]\n","epoch:8 loss:1.31e+00 pt:59.2 pf:64.1 svd:0 sample:64/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:43<25:01, 55.59s/epoch]\n","epoch:8 loss:1.31e+00 pt:58.7 pf:64.1 svd:0 sample:70/392 IAUC:54.1(B:53.7) IF1:67.6(B:68.7) PAUC:37.0(B:52.8) PF1:0.0(B:0.0) E:7(BE:1):  23%|██▎       | 8/35 [07:43<25:01, 55.59s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:01,  7.69s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:23,  3.35s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:11,  1.98s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:06,  1.33s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:03,  1.02batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.31batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.62batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.87batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.22batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:8 loss:1.31e+00 pt:58.7 pf:64.1 svd:0 sample:70/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  23%|██▎       | 8/35 [08:05<25:01, 55.59s/epoch]\n","epoch:8 loss:1.31e+00 pt:58.7 pf:64.1 svd:0 sample:70/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:05<23:24, 54.01s/epoch]\n","epoch:9 loss:1.31e+00 pt:56.4 pf:64.1 svd:0 sample:16/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:34<23:24, 54.01s/epoch]\n","epoch:9 loss:1.31e+00 pt:58.0 pf:64.8 svd:0 sample:32/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:35<23:24, 54.01s/epoch]\n","epoch:9 loss:1.30e+00 pt:59.0 pf:65.3 svd:0 sample:48/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:35<23:24, 54.01s/epoch]\n","epoch:9 loss:1.30e+00 pt:58.2 pf:66.0 svd:0 sample:64/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:36<23:24, 54.01s/epoch]\n","epoch:9 loss:1.30e+00 pt:57.8 pf:65.8 svd:0 sample:70/392 IAUC:53.1(B:53.7) IF1:67.0(B:68.7) PAUC:43.6(B:52.8) PF1:0.0(B:0.0) E:8(BE:1):  26%|██▌       | 9/35 [08:36<23:24, 54.01s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:24, 10.51s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:31,  4.50s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.58s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.69s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.20s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.09batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.37batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.59batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:13<00:00,  1.91batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:9 loss:1.30e+00 pt:57.8 pf:65.8 svd:0 sample:70/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  26%|██▌       | 9/35 [09:02<23:24, 54.01s/epoch]\n","epoch:9 loss:1.30e+00 pt:57.8 pf:65.8 svd:0 sample:70/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:03<22:53, 54.96s/epoch]\n","epoch:10 loss:1.29e+00 pt:58.6 pf:67.9 svd:0 sample:16/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:29<22:53, 54.96s/epoch]\n","epoch:10 loss:1.29e+00 pt:59.3 pf:67.8 svd:0 sample:32/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:30<22:53, 54.96s/epoch]\n","epoch:10 loss:1.29e+00 pt:59.8 pf:68.4 svd:0 sample:48/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:30<22:53, 54.96s/epoch]\n","epoch:10 loss:1.28e+00 pt:60.4 pf:68.6 svd:0 sample:64/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:31<22:53, 54.96s/epoch]\n","epoch:10 loss:1.28e+00 pt:60.4 pf:69.0 svd:0 sample:70/392 IAUC:64.4(B:53.7) IF1:69.0(B:68.7) PAUC:41.6(B:52.8) PF1:0.0(B:0.0) E:9(BE:1):  29%|██▊       | 10/35 [09:31<22:53, 54.96s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:19,  9.99s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.28s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.47s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:08,  1.61s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.14s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.18batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.49batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.83batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.19batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:10 loss:1.28e+00 pt:60.4 pf:69.0 svd:0 sample:70/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  29%|██▊       | 10/35 [09:56<22:53, 54.96s/epoch]\n","epoch:10 loss:1.28e+00 pt:60.4 pf:69.0 svd:0 sample:70/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [09:56<21:46, 54.44s/epoch]\n","epoch:11 loss:1.28e+00 pt:60.0 pf:70.6 svd:0 sample:16/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [10:25<21:46, 54.44s/epoch]\n","epoch:11 loss:1.28e+00 pt:61.3 pf:69.2 svd:0 sample:32/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [10:26<21:46, 54.44s/epoch]\n","epoch:11 loss:1.28e+00 pt:61.9 pf:69.6 svd:0 sample:48/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [10:26<21:46, 54.44s/epoch]\n","epoch:11 loss:1.27e+00 pt:62.0 pf:70.0 svd:0 sample:64/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [10:27<21:46, 54.44s/epoch]\n","epoch:11 loss:1.27e+00 pt:62.0 pf:70.3 svd:0 sample:70/392 IAUC:66.6(B:66.6) IF1:68.7(B:68.7) PAUC:46.6(B:46.6) PF1:0.0(B:0.0) E:10(BE:10):  31%|███▏      | 11/35 [10:27<21:46, 54.44s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:18,  9.78s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.24s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.47s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.79s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.25s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.08batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.39batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.71batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.10batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:11 loss:1.27e+00 pt:62.0 pf:70.3 svd:0 sample:70/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  31%|███▏      | 11/35 [10:53<21:46, 54.44s/epoch]\n","epoch:11 loss:1.27e+00 pt:62.0 pf:70.3 svd:0 sample:70/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [10:53<21:09, 55.18s/epoch]\n","epoch:12 loss:1.25e+00 pt:65.3 pf:71.3 svd:0 sample:16/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [11:21<21:09, 55.18s/epoch]\n","epoch:12 loss:1.26e+00 pt:62.1 pf:70.3 svd:0 sample:32/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [11:21<21:09, 55.18s/epoch]\n","epoch:12 loss:1.26e+00 pt:62.3 pf:69.3 svd:0 sample:48/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [11:22<21:09, 55.18s/epoch]\n","epoch:12 loss:1.26e+00 pt:63.1 pf:69.9 svd:0 sample:64/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [11:22<21:09, 55.18s/epoch]\n","epoch:12 loss:1.26e+00 pt:62.4 pf:68.9 svd:0 sample:70/392 IAUC:61.3(B:66.6) IF1:68.4(B:68.7) PAUC:42.3(B:46.6) PF1:0.0(B:0.0) E:11(BE:10):  34%|███▍      | 12/35 [11:22<21:09, 55.18s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:03,  7.94s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:24,  3.47s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.04s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:06,  1.38s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:03,  1.00batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.29batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.58batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.86batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.16batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:12 loss:1.26e+00 pt:62.4 pf:68.9 svd:0 sample:70/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  34%|███▍      | 12/35 [11:45<21:09, 55.18s/epoch]\n","epoch:12 loss:1.26e+00 pt:62.4 pf:68.9 svd:0 sample:70/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [11:45<19:53, 54.25s/epoch]\n","epoch:13 loss:1.24e+00 pt:66.0 pf:71.8 svd:0 sample:16/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [12:10<19:53, 54.25s/epoch]\n","epoch:13 loss:1.24e+00 pt:66.2 pf:71.1 svd:0 sample:32/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [12:11<19:53, 54.25s/epoch]\n","epoch:13 loss:1.24e+00 pt:65.1 pf:71.4 svd:0 sample:48/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [12:12<19:53, 54.25s/epoch]\n","epoch:13 loss:1.25e+00 pt:64.5 pf:69.9 svd:0 sample:64/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [12:12<19:53, 54.25s/epoch]\n","epoch:13 loss:1.24e+00 pt:64.5 pf:70.6 svd:0 sample:70/392 IAUC:65.6(B:65.6) IF1:69.7(B:69.7) PAUC:50.9(B:50.9) PF1:0.0(B:0.0) E:12(BE:12):  37%|███▋      | 13/35 [12:12<19:53, 54.25s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:13,  9.24s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:09<00:27,  3.98s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:09<00:13,  2.30s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:07,  1.51s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:10<00:04,  1.08s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:10<00:02,  1.24batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.56batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:11<00:00,  1.89batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:11<00:00,  2.30batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:13 loss:1.24e+00 pt:64.5 pf:70.6 svd:0 sample:70/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  37%|███▋      | 13/35 [12:36<19:53, 54.25s/epoch]\n","epoch:13 loss:1.24e+00 pt:64.5 pf:70.6 svd:0 sample:70/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [12:36<18:42, 53.44s/epoch]\n","epoch:14 loss:1.23e+00 pt:66.7 pf:72.6 svd:0 sample:16/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [13:07<18:42, 53.44s/epoch]\n","epoch:14 loss:1.23e+00 pt:65.4 pf:71.4 svd:0 sample:32/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [13:08<18:42, 53.44s/epoch]\n","epoch:14 loss:1.23e+00 pt:65.5 pf:72.6 svd:0 sample:48/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [13:08<18:42, 53.44s/epoch]\n","epoch:14 loss:1.24e+00 pt:65.0 pf:70.2 svd:0 sample:64/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [13:09<18:42, 53.44s/epoch]\n","epoch:14 loss:1.23e+00 pt:65.4 pf:71.6 svd:0 sample:70/392 IAUC:59.2(B:65.6) IF1:71.2(B:69.7) PAUC:46.8(B:50.9) PF1:0.0(B:0.0) E:13(BE:12):  40%|████      | 14/35 [13:09<18:42, 53.44s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:25, 10.67s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:32,  4.59s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.65s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.75s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:12<00:04,  1.24s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.07batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.35batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:13<00:00,  1.63batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:13<00:00,  1.97batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:14 loss:1.23e+00 pt:65.4 pf:71.6 svd:0 sample:70/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  40%|████      | 14/35 [13:37<18:42, 53.44s/epoch]\n","epoch:14 loss:1.23e+00 pt:65.4 pf:71.6 svd:0 sample:70/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [13:37<18:34, 55.74s/epoch]\n","epoch:15 loss:1.22e+00 pt:63.1 pf:73.3 svd:0 sample:16/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [14:03<18:34, 55.74s/epoch]\n","epoch:15 loss:1.22e+00 pt:64.5 pf:73.8 svd:0 sample:32/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [14:03<18:34, 55.74s/epoch]\n","epoch:15 loss:1.21e+00 pt:65.1 pf:73.1 svd:0 sample:48/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [14:04<18:34, 55.74s/epoch]\n","epoch:15 loss:1.21e+00 pt:66.7 pf:73.3 svd:0 sample:64/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [14:05<18:34, 55.74s/epoch]\n","epoch:15 loss:1.22e+00 pt:64.3 pf:71.9 svd:0 sample:70/392 IAUC:70.7(B:70.7) IF1:74.0(B:74.0) PAUC:59.0(B:59.0) PF1:0.1(B:0.1) E:14(BE:14):  43%|████▎     | 15/35 [14:05<18:34, 55.74s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:15,  9.39s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:09<00:28,  4.06s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.35s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:07,  1.54s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:10<00:04,  1.10s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:10<00:02,  1.21batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.53batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:11<00:00,  1.86batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:11<00:00,  2.23batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:15 loss:1.22e+00 pt:64.3 pf:71.9 svd:0 sample:70/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  43%|████▎     | 15/35 [14:30<18:34, 55.74s/epoch]\n","epoch:15 loss:1.22e+00 pt:64.3 pf:71.9 svd:0 sample:70/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:30<17:19, 54.72s/epoch]\n","epoch:16 loss:1.20e+00 pt:68.6 pf:74.3 svd:0 sample:16/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:55<17:19, 54.72s/epoch]\n","epoch:16 loss:1.21e+00 pt:67.2 pf:73.4 svd:0 sample:32/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:55<17:19, 54.72s/epoch]\n","epoch:16 loss:1.21e+00 pt:67.0 pf:71.7 svd:0 sample:48/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:56<17:19, 54.72s/epoch]\n","epoch:16 loss:1.21e+00 pt:66.2 pf:71.9 svd:0 sample:64/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:56<17:19, 54.72s/epoch]\n","epoch:16 loss:1.22e+00 pt:64.7 pf:72.7 svd:0 sample:70/392 IAUC:63.9(B:70.7) IF1:70.8(B:74.0) PAUC:44.6(B:59.0) PF1:0.8(B:0.1) E:15(BE:14):  46%|████▌     | 16/35 [14:57<17:19, 54.72s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:01,  7.67s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:07<00:23,  3.34s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:11,  1.94s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:07,  1.42s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.01s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.30batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.60batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.86batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.18batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:16 loss:1.22e+00 pt:64.7 pf:72.7 svd:0 sample:70/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  46%|████▌     | 16/35 [15:19<17:19, 54.72s/epoch]\n","epoch:16 loss:1.22e+00 pt:64.7 pf:72.7 svd:0 sample:70/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:19<15:56, 53.15s/epoch]\n","epoch:17 loss:1.21e+00 pt:62.4 pf:72.8 svd:0 sample:16/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:44<15:56, 53.15s/epoch]\n","epoch:17 loss:1.20e+00 pt:66.3 pf:73.3 svd:0 sample:32/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:45<15:56, 53.15s/epoch]\n","epoch:17 loss:1.20e+00 pt:67.4 pf:73.5 svd:0 sample:48/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:46<15:56, 53.15s/epoch]\n","epoch:17 loss:1.19e+00 pt:68.3 pf:73.8 svd:0 sample:64/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:48<15:56, 53.15s/epoch]\n","epoch:17 loss:1.19e+00 pt:68.1 pf:73.6 svd:0 sample:70/392 IAUC:63.3(B:70.7) IF1:70.4(B:74.0) PAUC:54.1(B:59.0) PF1:4.3(B:0.1) E:16(BE:14):  49%|████▊     | 17/35 [15:49<15:56, 53.15s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:25, 10.65s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:31,  4.56s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.62s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.70s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.20s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.13batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.43batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.76batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.14batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:17 loss:1.19e+00 pt:68.1 pf:73.6 svd:0 sample:70/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  49%|████▊     | 17/35 [16:14<15:56, 53.15s/epoch]\n","epoch:17 loss:1.19e+00 pt:68.1 pf:73.6 svd:0 sample:70/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:14<15:12, 53.69s/epoch]\n","epoch:18 loss:1.16e+00 pt:74.6 pf:77.7 svd:0 sample:16/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:38<15:12, 53.69s/epoch]\n","epoch:18 loss:1.18e+00 pt:67.8 pf:74.3 svd:0 sample:32/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:39<15:12, 53.69s/epoch]\n","epoch:18 loss:1.18e+00 pt:66.6 pf:74.2 svd:0 sample:48/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:39<15:12, 53.69s/epoch]\n","epoch:18 loss:1.18e+00 pt:67.4 pf:74.3 svd:0 sample:64/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:40<15:12, 53.69s/epoch]\n","epoch:18 loss:1.19e+00 pt:67.5 pf:73.0 svd:0 sample:70/392 IAUC:65.2(B:70.7) IF1:70.1(B:74.0) PAUC:46.7(B:59.0) PF1:0.1(B:0.1) E:17(BE:14):  51%|█████▏    | 18/35 [16:40<15:12, 53.69s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:04,  8.04s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:24,  3.48s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.03s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:06,  1.34s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:03,  1.03batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.36batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.70batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  2.00batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.39batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:18 loss:1.19e+00 pt:67.5 pf:73.0 svd:0 sample:70/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  51%|█████▏    | 18/35 [17:03<15:12, 53.69s/epoch]\n","epoch:18 loss:1.19e+00 pt:67.5 pf:73.0 svd:0 sample:70/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:03<13:56, 52.25s/epoch]\n","epoch:19 loss:1.19e+00 pt:69.8 pf:71.8 svd:0 sample:16/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:28<13:56, 52.25s/epoch]\n","epoch:19 loss:1.16e+00 pt:72.1 pf:75.3 svd:0 sample:32/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:32<13:56, 52.25s/epoch]\n","epoch:19 loss:1.19e+00 pt:67.3 pf:73.5 svd:0 sample:48/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:32<13:56, 52.25s/epoch]\n","epoch:19 loss:1.20e+00 pt:65.6 pf:73.7 svd:0 sample:64/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:33<13:56, 52.25s/epoch]\n","epoch:19 loss:1.19e+00 pt:66.9 pf:74.5 svd:0 sample:70/392 IAUC:82.6(B:82.6) IF1:83.9(B:83.9) PAUC:66.3(B:66.3) PF1:4.9(B:4.9) E:18(BE:18):  54%|█████▍    | 19/35 [17:33<13:56, 52.25s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:13<01:50, 13.80s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:14<00:41,  5.88s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:14<00:19,  3.33s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:14<00:10,  2.14s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:15<00:05,  1.48s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:15<00:03,  1.08s/batch]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:15<00:01,  1.22batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:15<00:00,  1.51batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:16<00:00,  1.89batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:19 loss:1.19e+00 pt:66.9 pf:74.5 svd:0 sample:70/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  54%|█████▍    | 19/35 [18:01<13:56, 52.25s/epoch]\n","epoch:19 loss:1.19e+00 pt:66.9 pf:74.5 svd:0 sample:70/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:01<13:28, 53.89s/epoch]\n","epoch:20 loss:1.21e+00 pt:63.2 pf:69.2 svd:0 sample:16/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:24<13:28, 53.89s/epoch]\n","epoch:20 loss:1.21e+00 pt:65.6 pf:68.0 svd:0 sample:32/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:27<13:28, 53.89s/epoch]\n","epoch:20 loss:1.19e+00 pt:67.1 pf:70.3 svd:0 sample:48/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:27<13:28, 53.89s/epoch]\n","epoch:20 loss:1.18e+00 pt:67.5 pf:71.7 svd:0 sample:64/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:28<13:28, 53.89s/epoch]\n","epoch:20 loss:1.18e+00 pt:67.4 pf:72.1 svd:0 sample:70/392 IAUC:72.3(B:82.6) IF1:71.6(B:83.9) PAUC:64.3(B:66.3) PF1:6.4(B:4.9) E:19(BE:18):  57%|█████▋    | 20/35 [18:28<13:28, 53.89s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:15,  9.47s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:09<00:28,  4.10s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.41s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:07,  1.60s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:10<00:04,  1.16s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.13batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.39batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.67batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  1.96batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:20 loss:1.18e+00 pt:67.4 pf:72.1 svd:0 sample:70/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  57%|█████▋    | 20/35 [18:49<13:28, 53.89s/epoch]\n","epoch:20 loss:1.18e+00 pt:67.4 pf:72.1 svd:0 sample:70/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [18:49<12:11, 52.28s/epoch]\n","epoch:21 loss:1.19e+00 pt:63.0 pf:74.5 svd:0 sample:16/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [19:15<12:11, 52.28s/epoch]\n","epoch:21 loss:1.16e+00 pt:66.8 pf:76.4 svd:0 sample:32/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [19:16<12:11, 52.28s/epoch]\n","epoch:21 loss:1.15e+00 pt:69.1 pf:77.1 svd:0 sample:48/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [19:17<12:11, 52.28s/epoch]\n","epoch:21 loss:1.15e+00 pt:69.3 pf:75.3 svd:0 sample:64/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [19:17<12:11, 52.28s/epoch]\n","epoch:21 loss:1.15e+00 pt:69.2 pf:74.9 svd:0 sample:70/392 IAUC:64.6(B:82.6) IF1:69.0(B:83.9) PAUC:53.7(B:66.3) PF1:5.0(B:4.9) E:20(BE:18):  60%|██████    | 21/35 [19:17<12:11, 52.28s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:18,  9.83s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.22s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.42s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.71s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.20s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.11batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.43batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.74batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.12batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:21 loss:1.15e+00 pt:69.2 pf:74.9 svd:0 sample:70/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  60%|██████    | 21/35 [19:43<12:11, 52.28s/epoch]\n","epoch:21 loss:1.15e+00 pt:69.2 pf:74.9 svd:0 sample:70/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [19:43<11:26, 52.78s/epoch]\n","epoch:22 loss:1.11e+00 pt:75.9 pf:79.3 svd:0 sample:16/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [20:08<11:26, 52.78s/epoch]\n","epoch:22 loss:1.13e+00 pt:71.2 pf:77.9 svd:0 sample:32/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [20:08<11:26, 52.78s/epoch]\n","epoch:22 loss:1.14e+00 pt:68.2 pf:76.8 svd:0 sample:48/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [20:09<11:26, 52.78s/epoch]\n","epoch:22 loss:1.13e+00 pt:69.9 pf:77.6 svd:0 sample:64/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [20:09<11:26, 52.78s/epoch]\n","epoch:22 loss:1.13e+00 pt:69.3 pf:76.5 svd:0 sample:70/392 IAUC:73.8(B:82.6) IF1:74.1(B:83.9) PAUC:55.6(B:66.3) PF1:0.5(B:4.9) E:21(BE:18):  63%|██████▎   | 22/35 [20:10<11:26, 52.78s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:18,  9.83s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.23s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.44s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:07,  1.59s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.13s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.19batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.48batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:11<00:00,  1.77batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.04batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:22 loss:1.13e+00 pt:69.3 pf:76.5 svd:0 sample:70/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  63%|██████▎   | 22/35 [20:33<11:26, 52.78s/epoch]\n","epoch:22 loss:1.13e+00 pt:69.3 pf:76.5 svd:0 sample:70/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [20:33<10:22, 51.87s/epoch]\n","epoch:23 loss:1.13e+00 pt:71.3 pf:75.7 svd:0 sample:16/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [20:59<10:22, 51.87s/epoch]\n","epoch:23 loss:1.11e+00 pt:73.6 pf:78.1 svd:0 sample:32/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [21:00<10:22, 51.87s/epoch]\n","epoch:23 loss:1.11e+00 pt:73.6 pf:79.0 svd:0 sample:48/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [21:01<10:22, 51.87s/epoch]\n","epoch:23 loss:1.10e+00 pt:74.7 pf:80.0 svd:0 sample:64/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [21:01<10:22, 51.87s/epoch]\n","epoch:23 loss:1.10e+00 pt:74.0 pf:78.8 svd:0 sample:70/392 IAUC:75.5(B:82.6) IF1:78.4(B:83.9) PAUC:67.6(B:66.3) PF1:7.7(B:4.9) E:22(BE:18):  66%|██████▌   | 23/35 [21:01<10:22, 51.87s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:22, 10.30s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:30,  4.41s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:15,  2.54s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.65s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.16s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.14batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.46batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.76batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.15batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:23 loss:1.10e+00 pt:74.0 pf:78.8 svd:0 sample:70/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  66%|██████▌   | 23/35 [21:25<10:22, 51.87s/epoch]\n","epoch:23 loss:1.10e+00 pt:74.0 pf:78.8 svd:0 sample:70/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:26<09:32, 52.07s/epoch]\n","epoch:24 loss:1.08e+00 pt:78.4 pf:81.3 svd:0 sample:16/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:52<09:32, 52.07s/epoch]\n","epoch:24 loss:1.08e+00 pt:76.1 pf:81.2 svd:0 sample:32/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:52<09:32, 52.07s/epoch]\n","epoch:24 loss:1.07e+00 pt:76.8 pf:82.2 svd:0 sample:48/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:53<09:32, 52.07s/epoch]\n","epoch:24 loss:1.07e+00 pt:76.6 pf:82.6 svd:0 sample:64/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:54<09:32, 52.07s/epoch]\n","epoch:24 loss:1.07e+00 pt:76.2 pf:82.9 svd:0 sample:70/392 IAUC:76.4(B:82.6) IF1:76.5(B:83.9) PAUC:64.2(B:66.3) PF1:4.8(B:4.9) E:23(BE:18):  69%|██████▊   | 24/35 [21:54<09:32, 52.07s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:19,  9.96s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:30,  4.31s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.50s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.66s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.19s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.09batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.36batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.60batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  1.91batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:24 loss:1.07e+00 pt:76.2 pf:82.9 svd:0 sample:70/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  69%|██████▊   | 24/35 [22:19<09:32, 52.07s/epoch]\n","epoch:24 loss:1.07e+00 pt:76.2 pf:82.9 svd:0 sample:70/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:19<08:45, 52.57s/epoch]\n","epoch:25 loss:1.09e+00 pt:81.3 pf:82.7 svd:0 sample:16/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:45<08:45, 52.57s/epoch]\n","epoch:25 loss:1.07e+00 pt:80.1 pf:83.3 svd:0 sample:32/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:46<08:45, 52.57s/epoch]\n","epoch:25 loss:1.06e+00 pt:80.3 pf:84.4 svd:0 sample:48/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:46<08:45, 52.57s/epoch]\n","epoch:25 loss:1.05e+00 pt:79.6 pf:84.6 svd:0 sample:64/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:47<08:45, 52.57s/epoch]\n","epoch:25 loss:1.05e+00 pt:80.0 pf:84.9 svd:0 sample:70/392 IAUC:80.1(B:80.1) IF1:78.9(B:78.9) PAUC:73.6(B:73.6) PF1:7.6(B:7.6) E:24(BE:24):  71%|███████▏  | 25/35 [22:47<08:45, 52.57s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:27, 10.90s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:11<00:32,  4.66s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:16,  2.67s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.73s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:12<00:04,  1.22s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.11batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.41batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.75batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:13<00:00,  2.11batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:25 loss:1.05e+00 pt:80.0 pf:84.9 svd:0 sample:70/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  71%|███████▏  | 25/35 [23:14<08:45, 52.57s/epoch]\n","epoch:25 loss:1.05e+00 pt:80.0 pf:84.9 svd:0 sample:70/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:14<07:57, 53.11s/epoch]\n","epoch:26 loss:1.05e+00 pt:77.8 pf:82.6 svd:0 sample:16/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:34<07:57, 53.11s/epoch]\n","epoch:26 loss:1.04e+00 pt:78.7 pf:83.1 svd:0 sample:32/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:37<07:57, 53.11s/epoch]\n","epoch:26 loss:1.03e+00 pt:80.2 pf:84.1 svd:0 sample:48/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:38<07:57, 53.11s/epoch]\n","epoch:26 loss:1.03e+00 pt:80.9 pf:84.9 svd:0 sample:64/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:38<07:57, 53.11s/epoch]\n","epoch:26 loss:1.03e+00 pt:80.9 pf:84.7 svd:0 sample:70/392 IAUC:72.3(B:80.1) IF1:77.2(B:78.9) PAUC:55.0(B:73.6) PF1:0.1(B:7.6) E:25(BE:24):  74%|███████▍  | 26/35 [23:39<07:57, 53.11s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:05,  8.15s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:24,  3.53s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.05s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:07,  1.48s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.05s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.25batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.58batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.89batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.27batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:26 loss:1.03e+00 pt:80.9 pf:84.7 svd:0 sample:70/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  74%|███████▍  | 26/35 [24:02<07:57, 53.11s/epoch]\n","epoch:26 loss:1.03e+00 pt:80.9 pf:84.7 svd:0 sample:70/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:02<06:53, 51.63s/epoch]\n","epoch:27 loss:1.02e+00 pt:82.6 pf:87.9 svd:0 sample:16/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:30<06:53, 51.63s/epoch]\n","epoch:27 loss:1.00e+00 pt:83.8 pf:88.8 svd:0 sample:32/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:30<06:53, 51.63s/epoch]\n","epoch:27 loss:1.02e+00 pt:81.4 pf:85.7 svd:0 sample:48/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:31<06:53, 51.63s/epoch]\n","epoch:27 loss:1.01e+00 pt:81.9 pf:85.9 svd:0 sample:64/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:31<06:53, 51.63s/epoch]\n","epoch:27 loss:1.01e+00 pt:82.7 pf:86.5 svd:0 sample:70/392 IAUC:81.8(B:80.1) IF1:79.7(B:78.9) PAUC:71.1(B:73.6) PF1:7.6(B:7.6) E:26(BE:24):  77%|███████▋  | 27/35 [24:31<06:53, 51.63s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:19,  9.97s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.28s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.46s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:08,  1.61s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.14s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.18batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.49batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.81batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.17batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:27 loss:1.01e+00 pt:82.7 pf:86.5 svd:0 sample:70/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  77%|███████▋  | 27/35 [24:57<06:53, 51.63s/epoch]\n","epoch:27 loss:1.01e+00 pt:82.7 pf:86.5 svd:0 sample:70/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [24:57<06:07, 52.57s/epoch]\n","epoch:28 loss:1.02e+00 pt:78.1 pf:84.6 svd:0 sample:16/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [25:18<06:07, 52.57s/epoch]\n","epoch:28 loss:1.00e+00 pt:81.3 pf:87.4 svd:0 sample:32/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [25:19<06:07, 52.57s/epoch]\n","epoch:28 loss:9.90e-01 pt:83.0 pf:88.4 svd:0 sample:48/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [25:20<06:07, 52.57s/epoch]\n","epoch:28 loss:9.84e-01 pt:83.7 pf:88.1 svd:0 sample:64/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [25:20<06:07, 52.57s/epoch]\n","epoch:28 loss:9.77e-01 pt:85.0 pf:88.7 svd:0 sample:70/392 IAUC:79.2(B:80.1) IF1:76.8(B:78.9) PAUC:66.2(B:73.6) PF1:6.1(B:7.6) E:27(BE:24):  80%|████████  | 28/35 [25:20<06:07, 52.57s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:08,  8.54s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:25,  3.68s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:09<00:12,  2.15s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:07,  1.43s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.05s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:10<00:02,  1.25batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.53batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.83batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:11<00:00,  2.22batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:28 loss:9.77e-01 pt:85.0 pf:88.7 svd:0 sample:70/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  80%|████████  | 28/35 [25:44<06:07, 52.57s/epoch]\n","epoch:28 loss:9.77e-01 pt:85.0 pf:88.7 svd:0 sample:70/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [25:44<05:05, 51.00s/epoch]\n","epoch:29 loss:9.63e-01 pt:87.1 pf:90.5 svd:0 sample:16/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [26:09<05:05, 51.00s/epoch]\n","epoch:29 loss:9.56e-01 pt:86.7 pf:90.6 svd:0 sample:32/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [26:10<05:05, 51.00s/epoch]\n","epoch:29 loss:9.51e-01 pt:88.1 pf:90.9 svd:0 sample:48/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [26:11<05:05, 51.00s/epoch]\n","epoch:29 loss:9.51e-01 pt:87.8 pf:90.6 svd:0 sample:64/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [26:11<05:05, 51.00s/epoch]\n","epoch:29 loss:9.51e-01 pt:87.4 pf:90.5 svd:0 sample:70/392 IAUC:75.4(B:80.1) IF1:74.2(B:78.9) PAUC:66.3(B:73.6) PF1:8.0(B:7.6) E:28(BE:24):  83%|████████▎ | 29/35 [26:11<05:05, 51.00s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:10,  8.83s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:09<00:26,  3.85s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:09<00:13,  2.25s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:07,  1.50s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:10<00:04,  1.08s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:10<00:02,  1.20batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.49batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:11<00:00,  1.78batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:11<00:00,  2.18batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:29 loss:9.51e-01 pt:87.4 pf:90.5 svd:0 sample:70/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  83%|████████▎ | 29/35 [26:36<05:05, 51.00s/epoch]\n","epoch:29 loss:9.51e-01 pt:87.4 pf:90.5 svd:0 sample:70/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:36<04:16, 51.31s/epoch]\n","epoch:30 loss:9.29e-01 pt:91.1 pf:92.2 svd:0 sample:16/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:56<04:16, 51.31s/epoch]\n","epoch:30 loss:9.36e-01 pt:90.3 pf:91.5 svd:0 sample:32/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:57<04:16, 51.31s/epoch]\n","epoch:30 loss:9.29e-01 pt:90.2 pf:91.4 svd:0 sample:48/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:59<04:16, 51.31s/epoch]\n","epoch:30 loss:9.29e-01 pt:89.1 pf:91.2 svd:0 sample:64/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:59<04:16, 51.31s/epoch]\n","epoch:30 loss:9.27e-01 pt:88.6 pf:91.2 svd:0 sample:70/392 IAUC:78.6(B:80.1) IF1:79.2(B:78.9) PAUC:71.4(B:73.6) PF1:10.2(B:7.6) E:29(BE:24):  86%|████████▌ | 30/35 [26:59<04:16, 51.31s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:10<01:24, 10.53s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:31,  4.50s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:11<00:15,  2.59s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:11<00:08,  1.68s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.19s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:12<00:02,  1.14batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:12<00:01,  1.45batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.77batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.13batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:30 loss:9.27e-01 pt:88.6 pf:91.2 svd:0 sample:70/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  86%|████████▌ | 30/35 [27:26<04:16, 51.31s/epoch]\n","epoch:30 loss:9.27e-01 pt:88.6 pf:91.2 svd:0 sample:70/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:26<03:23, 50.93s/epoch]\n","epoch:31 loss:8.77e-01 pt:94.4 pf:93.9 svd:0 sample:16/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:48<03:23, 50.93s/epoch]\n","epoch:31 loss:8.94e-01 pt:92.0 pf:93.0 svd:0 sample:32/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:50<03:23, 50.93s/epoch]\n","epoch:31 loss:9.02e-01 pt:91.8 pf:92.7 svd:0 sample:48/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:51<03:23, 50.93s/epoch]\n","epoch:31 loss:9.01e-01 pt:91.3 pf:92.4 svd:0 sample:64/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:52<03:23, 50.93s/epoch]\n","epoch:31 loss:9.00e-01 pt:91.1 pf:92.6 svd:0 sample:70/392 IAUC:77.1(B:80.1) IF1:77.0(B:78.9) PAUC:69.1(B:73.6) PF1:10.0(B:7.6) E:30(BE:24):  89%|████████▊ | 31/35 [27:52<03:23, 50.93s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:01,  7.69s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:23,  3.37s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:11,  1.98s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:07,  1.49s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.07s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.21batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.50batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.76batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.09batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:31 loss:9.00e-01 pt:91.1 pf:92.6 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  89%|████████▊ | 31/35 [28:18<03:23, 50.93s/epoch]\n","epoch:31 loss:9.00e-01 pt:91.1 pf:92.6 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:18<02:33, 51.22s/epoch]\n","epoch:32 loss:8.61e-01 pt:94.4 pf:94.5 svd:0 sample:16/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:41<02:33, 51.22s/epoch]\n","epoch:32 loss:8.72e-01 pt:93.3 pf:93.9 svd:0 sample:32/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:42<02:33, 51.22s/epoch]\n","epoch:32 loss:8.74e-01 pt:92.4 pf:94.1 svd:0 sample:48/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:42<02:33, 51.22s/epoch]\n","epoch:32 loss:8.76e-01 pt:92.0 pf:93.6 svd:0 sample:64/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:43<02:33, 51.22s/epoch]\n","epoch:32 loss:8.75e-01 pt:91.7 pf:93.4 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:79.0(B:78.9) PAUC:72.3(B:73.6) PF1:10.6(B:7.6) E:31(BE:24):  91%|█████████▏| 32/35 [28:43<02:33, 51.22s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:09<01:19,  9.96s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:10<00:29,  4.28s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:10<00:14,  2.46s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:10<00:08,  1.61s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:11<00:04,  1.13s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:11<00:02,  1.18batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:11<00:01,  1.48batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:12<00:00,  1.80batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:12<00:00,  2.16batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:32 loss:8.75e-01 pt:91.7 pf:93.4 svd:0 sample:70/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  91%|█████████▏| 32/35 [29:09<02:33, 51.22s/epoch] \n","epoch:32 loss:8.75e-01 pt:91.7 pf:93.4 svd:0 sample:70/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:09<01:42, 51.26s/epoch]\n","epoch:33 loss:8.85e-01 pt:94.2 pf:93.9 svd:0 sample:16/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:29<01:42, 51.26s/epoch]\n","epoch:33 loss:8.57e-01 pt:94.9 pf:94.0 svd:0 sample:32/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:29<01:42, 51.26s/epoch]\n","epoch:33 loss:8.54e-01 pt:94.8 pf:93.7 svd:0 sample:48/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:32<01:42, 51.26s/epoch]\n","epoch:33 loss:8.53e-01 pt:94.2 pf:93.8 svd:0 sample:64/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:33<01:42, 51.26s/epoch]\n","epoch:33 loss:8.42e-01 pt:94.5 pf:94.2 svd:0 sample:70/392 IAUC:78.7(B:80.1) IF1:76.8(B:78.9) PAUC:69.2(B:73.6) PF1:8.9(B:7.6) E:32(BE:24):  94%|█████████▍| 33/35 [29:33<01:42, 51.26s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:07<01:01,  7.71s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:07<00:23,  3.34s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:11,  1.95s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:08<00:06,  1.30s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:08<00:03,  1.06batch/s]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.39batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:09<00:01,  1.71batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:09<00:00,  2.00batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.34batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:33 loss:8.42e-01 pt:94.5 pf:94.2 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  94%|█████████▍| 33/35 [29:55<01:42, 51.26s/epoch]\n","epoch:33 loss:8.42e-01 pt:94.5 pf:94.2 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [29:55<00:49, 49.54s/epoch]\n","epoch:34 loss:8.53e-01 pt:90.6 pf:92.7 svd:0 sample:16/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [30:19<00:49, 49.54s/epoch]\n","epoch:34 loss:8.22e-01 pt:93.2 pf:94.0 svd:0 sample:32/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [30:20<00:49, 49.54s/epoch]\n","epoch:34 loss:8.20e-01 pt:93.9 pf:94.7 svd:0 sample:48/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [30:20<00:49, 49.54s/epoch]\n","epoch:34 loss:8.25e-01 pt:93.9 pf:94.5 svd:0 sample:64/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [30:21<00:49, 49.54s/epoch]\n","epoch:34 loss:8.24e-01 pt:93.7 pf:94.1 svd:0 sample:70/392 IAUC:81.2(B:80.1) IF1:80.8(B:78.9) PAUC:71.4(B:73.6) PF1:10.5(B:7.6) E:33(BE:24):  97%|█████████▋| 34/35 [30:21<00:49, 49.54s/epoch]\n","\n","Inferring...:   0%|          | 0/9 [00:00<?, ?batch/s]\u001b[A\n","\n","Inferring...:  11%|█         | 1/9 [00:08<01:04,  8.05s/batch]\u001b[A\n","\n","Inferring...:  22%|██▏       | 2/9 [00:08<00:24,  3.52s/batch]\u001b[A\n","\n","Inferring...:  33%|███▎      | 3/9 [00:08<00:12,  2.06s/batch]\u001b[A\n","\n","Inferring...:  44%|████▍     | 4/9 [00:09<00:06,  1.39s/batch]\u001b[A\n","\n","Inferring...:  56%|█████▌    | 5/9 [00:09<00:04,  1.00s/batch]\u001b[A\n","\n","Inferring...:  67%|██████▋   | 6/9 [00:09<00:02,  1.28batch/s]\u001b[A\n","\n","Inferring...:  78%|███████▊  | 7/9 [00:10<00:01,  1.55batch/s]\u001b[A\n","\n","Inferring...:  89%|████████▉ | 8/9 [00:10<00:00,  1.80batch/s]\u001b[A\n","\n","Inferring...: 100%|██████████| 9/9 [00:10<00:00,  2.13batch/s]\u001b[A\n","\n","                                                              \u001b[A\n","epoch:34 loss:8.24e-01 pt:93.7 pf:94.1 svd:0 sample:70/392 IAUC:77.3(B:80.1) IF1:76.2(B:78.9) PAUC:67.6(B:73.6) PF1:0.9(B:7.6) E:34(BE:24):  97%|█████████▋| 34/35 [30:43<00:49, 49.54s/epoch] \n","epoch:34 loss:8.24e-01 pt:93.7 pf:94.1 svd:0 sample:70/392 IAUC:77.3(B:80.1) IF1:76.2(B:78.9) PAUC:67.6(B:73.6) PF1:0.9(B:7.6) E:34(BE:24): 100%|██████████| 35/35 [30:43<00:00, 49.11s/epoch]\n","epoch:34 loss:8.24e-01 pt:93.7 pf:94.1 svd:0 sample:70/392 IAUC:77.3(B:80.1) IF1:76.2(B:78.9) PAUC:67.6(B:73.6) PF1:0.9(B:7.6) E:34(BE:24): 100%|██████████| 35/35 [30:43<00:00, 52.67s/epoch]\n","\n"]}],"source":["#@title trainingi başlat\n","# train kodunu çalıştır\n","%cd /content/drive/MyDrive/GLASS_09_05_2025/GLASS/shell/\n","!conda run -n GLASS ./run-mvtec.sh"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOFpNiDSN1b8AFo8lsVRQoP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}